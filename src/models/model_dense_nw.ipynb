{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a38e49ff-48e2-4d22-8232-add20dc4cd8c",
   "metadata": {},
   "source": [
    "# MODELADO\n",
    "\n",
    "## Dense Newtork\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "<code> **Proyecto de Datos II** </code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7c845b-dd88-42f7-aba6-1e2778fa821a",
   "metadata": {},
   "source": [
    "## Índice\n",
    "\n",
    "- [Importación de los datos](#importación-de-los-datos)\n",
    "- [Preprocesamiento](#preprocesamiento)\n",
    "- [Entrenamiento](#entrenamiento)\n",
    "- [Análisis del modelo](#análisis-del-modelo)\n",
    "- [Registro del modelo en MLflow](#registro-del-modelo-en-mlflow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2f464da-fc1a-45ba-a080-6cfde2316aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from evaluation.evaluator import Evaluator\n",
    "\n",
    "SEED = 22 # replicabilidad\n",
    "\n",
    "# =====================================\n",
    "MODEL_NAME = \"dense_network\"\n",
    "# ====================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd637b55-3e74-410d-819b-6aa19ed9c30e",
   "metadata": {},
   "source": [
    "## Importación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dbff19-c15b-4730-8930-a3e558288eec",
   "metadata": {},
   "source": [
    "Se ha tomado del conjunto de datos de entrenamiento como validación del 1 al 14 de enero (los mensajes más recientes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeffee5d-bee7-47cb-ad3a-da90737097b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Cargamos los datos\n",
    "df_train = pd.read_parquet(\"../data/train.parquet\")\n",
    "df_test = pd.read_parquet(\"../data/test.parquet\")\n",
    "\n",
    "# Separamos en variable objetivo y features\n",
    "X_train_full = df_train.drop(columns=\"takeoff_time\")\n",
    "y_train_full = df_train[\"takeoff_time\"]\n",
    "\n",
    "X_test = df_test.drop(columns=\"takeoff_time\")\n",
    "y_test = df_test[\"takeoff_time\"]\n",
    "\n",
    "# Convertimos columna timestamp a datetime\n",
    "X_train_full[\"timestamp\"] = pd.to_datetime(X_train_full[\"timestamp\"])\n",
    "\n",
    "# Dividimos entre train y validation basado en fecha para evitar contaminación de futuro\n",
    "split_date = pd.to_datetime(\"2025-01-01\")\n",
    "train_mask = X_train_full[\"timestamp\"] < split_date\n",
    "val_mask = X_train_full[\"timestamp\"] >= split_date\n",
    "\n",
    "X_train = X_train_full[train_mask].drop(columns=[\"timestamp\", \"icao\", \"callsign\"])\n",
    "y_train = y_train_full[train_mask]\n",
    "\n",
    "X_val = X_train_full[val_mask].drop(columns=[\"timestamp\", \"icao\", \"callsign\"])\n",
    "y_val = y_train_full[val_mask]\n",
    "\n",
    "X_test = X_test.drop(columns=[\"timestamp\", \"icao\", \"callsign\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ea6d533-6fcb-4a43-93f9-0fccd38e6198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((98645, 57), (25088, 57), (27791, 57))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a847e73-4ee8-4336-9efb-d74bcbc788c9",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f577cf6-dfe3-47af-a655-758722bc1024",
   "metadata": {},
   "source": [
    "Se han normalizado las variables numéricas y aplicado One-Hot Encoding a las variables categóricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c64cfcc3-9d57-4491-997f-f33a70228e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - One-hot encoding -\n",
    "X_train = pd.get_dummies(X_train, drop_first=True)\n",
    "X_val = pd.get_dummies(X_val, drop_first=True)\n",
    "X_test = pd.get_dummies(X_test, drop_first=True)\n",
    "\n",
    "# Alineamos columnas por si falta alguna categoría\n",
    "X_val = X_val.reindex(columns=X_train.columns, fill_value=0)\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "# Escalamos las variables numéricas\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa5f499-81d5-4c41-b192-961e8a1dc485",
   "metadata": {},
   "source": [
    "Asimismo, también le hemos realizado una transformación logarítmica a la variable respuesta, lo cual mejora ligeramente los resultados del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77020682-9428-42d7-b7aa-50a08ce71c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 1. Función para transformar el target (log1p)\n",
    "def preprocess_target(y):\n",
    "    \"\"\"Aplica transformación logarítmica a los datos.\"\"\"\n",
    "    y_np = y.values.reshape(-1, 1)\n",
    "    y_log = np.log1p(y_np)\n",
    "    return y_log\n",
    "\n",
    "# 2. Función para revertir el preprocesamiento\n",
    "def inverse_preprocess_target(y_scaled, scaler):\n",
    "    \"\"\"Revierte MinMaxScaler y la transformación logarítmica.\"\"\"\n",
    "    y_log = scaler.inverse_transform(y_scaled)\n",
    "    y_original = np.expm1(y_log)\n",
    "    return y_original\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "y_train_log = preprocess_target(y_train)\n",
    "y_train_scaled = y_scaler.fit_transform(y_train_log)\n",
    "\n",
    "y_val_log = preprocess_target(y_val)\n",
    "y_val_scaled = y_scaler.transform(y_val_log)\n",
    "\n",
    "y_test_log = preprocess_target(y_test)\n",
    "y_test_scaled = y_scaler.transform(y_test_log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25078f0-34aa-4eca-acac-646f3b819823",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98e6f69a-b80b-45e9-8629-790298d53b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 01m 39s]\n",
      "val_mae: 0.12479636073112488\n",
      "\n",
      "Best val_mae So Far: 0.10553476959466934\n",
      "Total elapsed time: 00h 30m 59s\n",
      "\n",
      "Entrenando modelo final con mejores hiperparámetros...\n",
      "Epoch 1/100\n",
      "3083/3083 - 5s - 1ms/step - loss: 0.0637 - mae: 0.1551 - val_loss: 0.0232 - val_mae: 0.1136\n",
      "Epoch 2/100\n",
      "3083/3083 - 4s - 1ms/step - loss: 0.0178 - mae: 0.0999 - val_loss: 0.0228 - val_mae: 0.1119\n",
      "Epoch 3/100\n",
      "3083/3083 - 4s - 1ms/step - loss: 0.0126 - mae: 0.0872 - val_loss: 0.0215 - val_mae: 0.1104\n",
      "Epoch 4/100\n",
      "3083/3083 - 4s - 1ms/step - loss: 0.0098 - mae: 0.0769 - val_loss: 0.0233 - val_mae: 0.1145\n",
      "Epoch 5/100\n",
      "3083/3083 - 4s - 1ms/step - loss: 0.0081 - mae: 0.0697 - val_loss: 0.0240 - val_mae: 0.1158\n",
      "Epoch 6/100\n",
      "3083/3083 - 4s - 1ms/step - loss: 0.0069 - mae: 0.0640 - val_loss: 0.0231 - val_mae: 0.1143\n",
      "Epoch 7/100\n",
      "3083/3083 - 4s - 1ms/step - loss: 0.0060 - mae: 0.0595 - val_loss: 0.0243 - val_mae: 0.1170\n",
      "Epoch 8/100\n",
      "3083/3083 - 4s - 1ms/step - loss: 0.0054 - mae: 0.0561 - val_loss: 0.0238 - val_mae: 0.1158\n",
      "Epoch 9/100\n",
      "3083/3083 - 4s - 1ms/step - loss: 0.0049 - mae: 0.0537 - val_loss: 0.0244 - val_mae: 0.1165\n",
      "Epoch 10/100\n",
      "3083/3083 - 4s - 1ms/step - loss: 0.0045 - mae: 0.0512 - val_loss: 0.0251 - val_mae: 0.1189\n",
      "Epoch 11/100\n",
      "3083/3083 - 4s - 1ms/step - loss: 0.0043 - mae: 0.0499 - val_loss: 0.0239 - val_mae: 0.1161\n",
      "Epoch 12/100\n",
      "3083/3083 - 4s - 1ms/step - loss: 0.0040 - mae: 0.0482 - val_loss: 0.0247 - val_mae: 0.1183\n",
      "Epoch 13/100\n",
      "3083/3083 - 4s - 1ms/step - loss: 0.0039 - mae: 0.0471 - val_loss: 0.0247 - val_mae: 0.1183\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "import keras_tuner as kt\n",
    "import time\n",
    "\n",
    "# Registramos el tiempo de inicio para medir el tiempo total de ejecución\n",
    "start_time = time.time()\n",
    "\n",
    "# Definimos la función de construcción del modelo, que será usada por Keras Tuner\n",
    "def build_model(hp, input_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(input_shape,)))\n",
    "\n",
    "    # Seleccionamos aleatoriamente el número de capas ocultas (entre 1 y 3)\n",
    "    num_layers = hp.Int('num_layers', 1, 3)\n",
    "    for i in range(num_layers):\n",
    "        # Añadimos una capa densa con un número de unidades determinado por el optimizador\n",
    "        model.add(layers.Dense(\n",
    "            units=hp.Int(f'units_{i}', min_value=64, max_value=512, step=64),\n",
    "            activation='relu'\n",
    "        ))\n",
    "        \n",
    "        # Añadimos una capa Dropout para prevenir sobreajuste\n",
    "        model.add(layers.Dropout(rate=hp.Float(f'dropout_{i}', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "\n",
    "    # Añadimos la capa de salida con una sola neurona (regresión)\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    # Compilamos el modelo usando el optimizador Adam con tasa de aprendizaje y decaimiento ajustables\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=hp.Float('learning_rate', 1e-5, 1e-2, sampling='LOG'),\n",
    "            decay=hp.Float('lr_decay', 1e-6, 1e-3, sampling='LOG')\n",
    "        ),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Definimos la función que se encargará de ejecutar la búsqueda de hiperparámetros\n",
    "def tune_model(X_train_scaled, y_train, X_val_scaled, y_val):\n",
    "    tuner = kt.BayesianOptimization(\n",
    "        lambda hp: build_model(hp, X_train_scaled.shape[1]),\n",
    "        objective='val_mae',\n",
    "        max_trials=30,  # Número máximo de combinaciones que vamos a probar\n",
    "        directory='kt_tuning_dir_bayesian',\n",
    "        project_name='takeoff_time',\n",
    "        overwrite=True\n",
    "    )\n",
    "\n",
    "    # Añadimos una parada temprana para evitar sobreentrenamiento\n",
    "    early_stop = callbacks.EarlyStopping(monitor='val_mae', patience=5, restore_best_weights=True)\n",
    "\n",
    "    # Ejecutamos la búsqueda de hiperparámetros\n",
    "    tuner.search(\n",
    "        X_train_scaled, y_train_scaled,\n",
    "        validation_data=(X_val_scaled, y_val_scaled),\n",
    "        epochs=30,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    # Obtenemos la mejor combinación de hiperparámetros\n",
    "    best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    return best_hp, tuner\n",
    "\n",
    "# Llamamos al proceso de optimización\n",
    "print(\"\\nBuscando mejores hiperparámetros...\")\n",
    "best_hp, tuner = tune_model(X_train_scaled, y_train, X_val_scaled, y_val)\n",
    "\n",
    "# Entrenamos el modelo final usando los mejores hiperparámetros encontrados\n",
    "print(\"\\nEntrenando modelo final con mejores hiperparámetros...\")\n",
    "final_model = build_model(best_hp, X_train_scaled.shape[1])\n",
    "\n",
    "# Añadimos una parada temprana más estricta para el entrenamiento final\n",
    "early_stop_final = callbacks.EarlyStopping(monitor='val_mae', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Entrenamos el modelo final con más épocas\n",
    "history = final_model.fit(\n",
    "    X_train_scaled, y_train_scaled,\n",
    "    validation_data=(X_val_scaled, y_val_scaled),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop_final],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Registramos el tiempo de fin y calculamos la duración total\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7970bbdb-c953-4e17-8bb8-c99b1187dd03",
   "metadata": {},
   "source": [
    "## Análisis del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6cd1f60-b05f-4ef2-b806-15ac8b31e0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3083/3083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 314us/step\n",
      "\u001b[1m784/784\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325us/step\n",
      "MAE en entrenamiento: 51.8880\n",
      "RMSE en entrenamiento: 76.5503\n",
      "MAE en validación: 81.3606\n",
      "RMSE en validación: 134.9141\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Predicciones en el conjunto de entrenamiento\n",
    "y_train_pred_scaled = final_model.predict(X_train_scaled)\n",
    "y_train_pred = inverse_preprocess_target(y_train_pred_scaled, y_scaler)\n",
    "y_train_true = y_train.values.reshape(-1, 1)  # Asegurarse que y_train esté en (n, 1)\n",
    "\n",
    "# Predicciones en el conjunto de validación\n",
    "y_val_pred_scaled = final_model.predict(X_val_scaled)\n",
    "y_val_pred = inverse_preprocess_target(y_val_pred_scaled, y_scaler)\n",
    "y_val_true = y_val.values.reshape(-1, 1)\n",
    "\n",
    "# Calculamos las métricas\n",
    "mae_train = mean_absolute_error(y_train_true, y_train_pred)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train_true, y_train_pred))\n",
    "\n",
    "mae_val = mean_absolute_error(y_val_true, y_val_pred)\n",
    "rmse_val = np.sqrt(mean_squared_error(y_val_true, y_val_pred))\n",
    "\n",
    "# Mostramos los resultados\n",
    "print(f\"MAE en entrenamiento: {mae_train:.4f}\")\n",
    "print(f\"RMSE en entrenamiento: {rmse_train:.4f}\")\n",
    "print(f\"MAE en validación: {mae_val:.4f}\")\n",
    "print(f\"RMSE en validación: {rmse_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1469d262-f7d3-4167-9291-07bb4b685fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322us/step\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# Generamos predicciones en test (en escala transformada)\n",
    "y_pred_scaled = final_model.predict(X_test_scaled)\n",
    "\n",
    "# Invertimos la transformación para obtener las predicciones originales\n",
    "y_pred = inverse_preprocess_target(y_pred_scaled, y_scaler)\n",
    "\n",
    "df_test['prediction'] = y_pred.flatten()  # Flatten para que sea una columna 1D\n",
    "# ==============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18706da7-ae3b-4003-8bcb-6dc73a0194ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2a3039040>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Nota: df_test tiene que tener la columna 'prediction'\n",
    "ev = Evaluator(df_test, MODEL_NAME, mae_val, rmse_val)\n",
    "report = ev.getReport()\n",
    "ev.visualEvaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc79d07-4d85-4dba-8242-331e8102d0ca",
   "metadata": {},
   "source": [
    "## Registro del modelo en MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4654dd7-3e73-41b8-b7f8-eeb559fff34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/27 15:23:44 WARNING mlflow.keras.save: You are saving a Keras model without specifying model signature.\n",
      "\u001b[31m2025/04/27 15:23:52 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"file:./mlflow_experiments\")\n",
    "mlflow.set_experiment(\"takeoff_time_prediction\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "\n",
    "    # - Datos generales -\n",
    "\n",
    "    # ========================================================================\n",
    "    mlflow.set_tag(\"model_type\", MODEL_NAME)\n",
    "    mlflow.set_tag(\"framework\", \"tensorflow\") # scikit-learn, tensorflow, etc.\n",
    "    mlflow.set_tag(\"target_variable\", \"takeoff_time\") # variable respuesta\n",
    "    mlflow.set_tag(\"preprocessing\", \"standard scaler + log y scaler en y\") # transformaciones separadas por un +\n",
    "    mlflow.set_tag(\"dataset\", \"original\") # indicar si se ha modificado el conjunto de datos\n",
    "    mlflow.set_tag(\"seed\", SEED) # semilla para replicabilidad\n",
    "    # ========================================================================\n",
    "    \n",
    "    # - Hiperparámetros óptimos -\n",
    "    \n",
    "    # =====================================\n",
    "    if 'best_hp' in locals():\n",
    "        best_hp_dict = best_hp.values  # Convertir los mejores hiperparámetros a un diccionario\n",
    "\n",
    "        # Guardar automáticamente los hiperparámetros\n",
    "        mlflow.log_params(best_hp_dict)\n",
    "    \n",
    "    mlflow.log_param(\"model\", MODEL_NAME)\n",
    "    # =====================================\n",
    "    \n",
    "    # - Métricas -\n",
    "\n",
    "    mlflow.log_metric(\"execution_time_s\", execution_time)\n",
    "\n",
    "    mlflow.log_metric(\"mae_val\", mae_val)\n",
    "    mlflow.log_metric(\"rmse_val\", rmse_val)\n",
    "\n",
    "    mlflow.log_metric(\"mae_train\", mae_train)\n",
    "    mlflow.log_metric(\"rmse_train\", rmse_train)\n",
    "\n",
    "    # Registrar métricas globales en test\n",
    "    for metric_name, value in report[\"global\"].items():\n",
    "        mlflow.log_metric(f\"{metric_name}_test\", value)\n",
    "    \n",
    "    # Registrar métricas por runway\n",
    "    for runway, metrics in report[\"by_runway\"].items():\n",
    "        for metric_name, value in metrics.items():\n",
    "            mlflow.log_metric(f\"{metric_name}_test_runway_{runway}\", value)\n",
    "    \n",
    "    # Registrar métricas por holding point\n",
    "    for hp, metrics in report[\"by_holding_point\"].items():\n",
    "        for metric_name, value in metrics.items():\n",
    "            mlflow.log_metric(f\"{metric_name}_test_hp_{hp}\", value)\n",
    "\n",
    "    # - Modelo -\n",
    "\n",
    "    # ========================================================================\n",
    "    # NOTA - Dependiendo de con qué has hecho el modelo esto hay que cambiarlo\n",
    "    mlflow.keras.log_model(final_model, \"model\")\n",
    "    # ========================================================================\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0aa475d6-3fd2-41c8-9916-5ce17302c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Visualizar experimentos -\n",
    "# !mlflow ui --backend-store-uri ./mlflow_experiments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
