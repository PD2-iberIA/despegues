{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline_bucle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%md` not found.\n"
     ]
    }
   ],
   "source": [
    "%md\n",
    "# IberIA - Construcción del conjunto de datos final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "## Pipeline\n",
    "#### 1. Decodificamos mensajes y convertimos en dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 2. Separamos dataframes de posiciones, callsigns, velocidades y categorías de turbulencia. Decodificamos mensajes y convertimos en dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "def getPositions(df):\n",
    "        \"\"\"\n",
    "        Genera un DataFrame con los datos necesarios para visualizar las posiciones.\n",
    "        \n",
    "        Parámetros:\n",
    "            df: DataFrame de datos\n",
    "        Devuelve:\n",
    "            DataFrame con las siguientes columnas: \"Timestamp (date)\", \"ICAO\", \"Flight status\", \"lat\", \"lon\".\n",
    "        \"\"\"\n",
    "        df_pos = df.filter((F.col(\"Typecode\") >= 5) & (F.col(\"Typecode\") <= 22) & (F.col(\"Typecode\") != 19))\\\n",
    "               .select(\"Timestamp (date)\", \"ICAO\", \"Flight status\", \"lat\", \"lon\")\\\n",
    "               .distinct()\n",
    "        \n",
    "        return df_pos\n",
    "    \n",
    "def getAirplaneCategories(df):\n",
    "    \"\"\"\n",
    "    Genera un DataFrame con la categoría de cada aeronave.\n",
    "    \n",
    "    Parámetros:\n",
    "        df: DataFrame de datos.\n",
    "    Devuelve:\n",
    "        DataFrame con las siguientes columnas: \"ICAO\", \"TurbulenceCategory\".\n",
    "    \"\"\"\n",
    "    # Seleccionamos mensajes ADS-B\n",
    "    df_filtered = df.filter(F.col(\"Downlink Format\").isin([17, 18]))\n",
    "    # Nos quedamos con los ICAOs y su tipo de avión\n",
    "    df_filtered = df_filtered.filter(F.col(\"TurbulenceCategory\").isNotNull() & (F.col(\"TurbulenceCategory\") != \"None\") & (~F.isnan(F.col(\"TurbulenceCategory\"))))\n",
    "    df_result = df_filtered.select(\"ICAO\", \"TurbulenceCategory\").dropDuplicates()\n",
    "    return df_result\n",
    "def getFlights(df):\n",
    "    \"\"\"\n",
    "    Genera un DataFrame con los datos de vuelo.\n",
    "    \n",
    "    Parámetros:\n",
    "        df: DataFrame de datos.\n",
    "    Devuelve:\n",
    "        DataFrame con las siguientes columnas: \"Timestamp (date)\", \"ICAO\", \"Callsign\".\n",
    "    \"\"\"\n",
    "    NULL_CALLSIGN = \"########\"  # valor de nulo de la columna\n",
    "    # Seleccionamos las filas que contengan información relativa al identificador de vuelo\n",
    "    df_flights = df.filter((F.col(\"Callsign\").isNotNull()) & \n",
    "                           (F.col(\"Callsign\") != NULL_CALLSIGN) & \n",
    "                           (~F.isnan(F.col(\"Callsign\"))))\\\n",
    "               .select(\"Timestamp (date)\", \"ICAO\", \"Callsign\")\\\n",
    "               .distinct()  # Para evitar duplicados si es necesario\n",
    "    return df_flights\n",
    "    \n",
    "def getAltitudes(df):\n",
    "        \"\"\"\n",
    "        Genera un Dataframe con los datos necesarios para visualizar las altitudes.\n",
    "        \n",
    "        Parámetros:\n",
    "            df: DataFrame de datos.\n",
    "        Devuelve:\n",
    "            DataFrame con las siguientes columnas: \"Timestamp (date)\", \"ICAO\", \"Callsign\", \"Flight status\", \"Altitude (ft)\", \"lat\", \"lon\".\n",
    "        \"\"\"\n",
    "        # DataFrame filtrando las filas que contienen una altitud no nula\n",
    "        df_alt = df.filter(F.col(\"Altitude (ft)\").isNotNull()  & (~F.isnan(F.col(\"Speed\"))))\\\n",
    "               .select(\"Timestamp (date)\", \"ICAO\", \"Callsign\", \"Flight status\", \"Altitude (ft)\", \"lat\", \"lon\")\n",
    "        return df_alt\n",
    "        \n",
    "def getVelocities(df):\n",
    "    \"\"\"\n",
    "    Genera un dataframe con los mensajes relativos a la velocidad.\n",
    "    Parámetros:\n",
    "        df: DataFrame de datos.\n",
    "    Devuelve:\n",
    "        DataFrame con las siguientes columnas: \"Timestamp (date)\", \"ICAO\", \"Flight status\", \"Speed\", \"lat\", \"lon\".\n",
    "    \"\"\"\n",
    "    # Filtramos las filas donde la velocidad no es nula\n",
    "    df_vel = df.filter(F.col(\"Speed\").isNotNull() & (~F.isnan(F.col(\"Speed\"))))\\\n",
    "           .select(\"Timestamp (date)\", \"ICAO\", \"Flight status\", \"Speed\")\n",
    "    \n",
    "    return df_vel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 3. Filtramos para posiciones cercanas al aeropuerto (radio de 5km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, radians, sin, cos, acos, col\n",
    "AIRPORT_CENTER_LAT = 40.49291\n",
    "AIRPORT_CENTER_LON = -3.56974 \n",
    "RADIUS_AIRPORT = 5000\n",
    "def filter_positions_within_radius(df, lat_col=\"lat\", lon_col=\"lon\", lat_ref= AIRPORT_CENTER_LAT, lon_ref=AIRPORT_CENTER_LON, radius_m=RADIUS_AIRPORT):\n",
    "    \"\"\"\n",
    "    Filtra un DataFrame de PySpark para conservar solo los registros que están dentro de un radio específico\n",
    "    desde un punto de referencia, usando la fórmula de Haversine.\n",
    "    :param df: DataFrame de entrada con columnas de latitud y longitud\n",
    "    :param lat_col: Nombre de la columna de latitud en el DataFrame\n",
    "    :param lon_col: Nombre de la columna de longitud en el DataFrame\n",
    "    :param lat_ref: Latitud del punto de referencia\n",
    "    :param lon_ref: Longitud del punto de referencia\n",
    "    :param radius_m: Radio en metros para el filtrado (por defecto 5000 m)\n",
    "    :return: DataFrame filtrado con una columna adicional 'distance_ref'\n",
    "    \"\"\"\n",
    "    R = 6371000  # Radio de la Tierra en metros\n",
    "    # Calcular distancia\n",
    "    distance_expr = R * acos(\n",
    "        sin(radians(lit(lat_ref))) * sin(radians(col(lat_col))) +\n",
    "        cos(radians(lit(lat_ref))) * cos(radians(col(lat_col))) *\n",
    "        cos(radians(lit(lon_ref)) - radians(col(lon_col)))\n",
    "    )\n",
    "    # Calculamos la distancia con respecto al punto de referencia\n",
    "    df_with_dist = df.withColumn(\"distance_ref\", distance_expr)\n",
    "    # Nos quedamos solo con las posiciones que no superan la distancia radius_m\n",
    "    filtered_df = df_with_dist.filter(col(\"distance_ref\") <= radius_m)\n",
    "    # Eliminamos la columna 'distance_ref'\n",
    "    return filtered_df.drop(\"distance_ref\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 4. Combinar posiciones con callsigns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge de posiciones y estado de vuelo\n",
    "def combinePsitionsFlights(df_pos_airport, df_flights):\n",
    "    \n",
    "    TOLERANCE_CALLSIGN_ASSIGNMENT = 600  # 10 minutos en segundos\n",
    "    \n",
    "    df_pos_airport = df_pos_airport.withColumn(\"Timestamp_sec\", F.unix_timestamp(\"Timestamp (date)\"))\n",
    "    df_flights = df_flights.withColumn(\"Timestamp_sec\", F.unix_timestamp(\"Timestamp (date)\"))\n",
    "    \n",
    "    df_pos_airport = df_pos_airport.withColumnRenamed(\"Timestamp_sec\", \"Timestamp_sec_pos\") \\\n",
    "                                   .withColumnRenamed(\"Timestamp (date)\", \"Timestamp\")\n",
    "    \n",
    "    df_flights = df_flights.withColumnRenamed(\"Timestamp_sec\", \"Timestamp_sec_flight\") \\\n",
    "                           .withColumnRenamed(\"Timestamp (date)\", \"Timestamp_flight\")\n",
    "    \n",
    "    df_pos_callsign = df_pos_airport.join(df_flights, on=\"ICAO\", how=\"inner\").filter(\n",
    "        F.abs(df_pos_airport[\"Timestamp_sec_pos\"] - df_flights[\"Timestamp_sec_flight\"]) <= TOLERANCE_CALLSIGN_ASSIGNMENT\n",
    "    )\n",
    "    \n",
    "    df_pos_callsign = df_pos_callsign.drop(\"Timestamp_sec_flight\", \"Timestamp_sec_pos\", \"Timestamp_flight\")\n",
    "    df_pos_callsign = df_pos_callsign.dropDuplicates([\"ICAO\", \"Timestamp\"])\n",
    "    \n",
    "    return df_pos_callsign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 5. Detección de aeronaves situados en puntos de espera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "import math\n",
    "# Función para calcular la distancia de Haversine\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371000  # Radio de la Tierra en metros\n",
    "    lat1_rad = math.radians(float(lat1))\n",
    "    lon1_rad = math.radians(float(lon1))\n",
    "    lat2_rad = math.radians(float(lat2))\n",
    "    lon2_rad = math.radians(float(lon2))\n",
    "    \n",
    "    return R * math.acos(\n",
    "        math.sin(lat1_rad) * math.sin(lat2_rad) +\n",
    "        math.cos(lat1_rad) * math.cos(lat2_rad) * math.cos(lon1_rad - lon2_rad)\n",
    "    )\n",
    "# UDF para asignar Designator y Runway basado en la distancia\n",
    "def assign_designator_runway(lat, lon, holding_points):\n",
    "    # Inicializamos el valor de retorno como None\n",
    "    \n",
    "    DIST_THRESHOLD = 20\n",
    "    \n",
    "    closest_designator = None\n",
    "    closest_runway = None\n",
    "    min_distance = float(\"inf\")  # Inicializamos con un valor grande\n",
    "    \n",
    "    # Iteramos sobre los puntos de espera para encontrar el más cercano\n",
    "    for hold in holding_points:\n",
    "        designator, runway, lon_p, lat_p = hold\n",
    "        dist = haversine_distance(lat, lon, lat_p, lon_p)\n",
    "        \n",
    "        # Si la distancia es menor a 20 metros, devolvemos el primer punto\n",
    "        if dist < DIST_THRESHOLD:\n",
    "            closest_designator = designator\n",
    "            closest_runway = runway\n",
    "            break  # Detenemos la búsqueda una vez encontramos el primer punto cercano\n",
    "    \n",
    "    return (closest_designator, closest_runway)\n",
    "# Diccionario de puntos de espera\n",
    "holding_points = [\n",
    "    (\"Z1\", \"36L/18R\", -3.573093114831562, 40.490653160130186),\n",
    "    (\"KA6\", \"32R/14L\", -3.537524367869231, 40.472076292010001),\n",
    "    (\"KA8\", \"32R/14L\", -3.536653485337274, 40.466622566754253),\n",
    "    (\"K3\", \"32R/14L\", -3.558959606954449, 40.494122669084419),\n",
    "    (\"K2\", \"32R/14L\", -3.559326044131887, 40.4945961819448),\n",
    "    (\"K1\", \"32R/14L\", -3.560411408421098, 40.495554592925956),\n",
    "    (\"Y1\", \"36R/18L\", -3.560656492186808, 40.499431092287409),\n",
    "    (\"Y2\", \"36R/18L\", -3.560645785166937, 40.500298406173002),\n",
    "    (\"Y3\", \"36R/18L\", -3.560660061193443, 40.501183565039504),\n",
    "    (\"Y7\", \"36R/18L\", -3.560800449906033, 40.533391949745102),\n",
    "    (\"Z6\", \"36L/18R\", -3.576129307304151, 40.495184843931881),\n",
    "    (\"Z4\", \"36L/18R\", -3.576034129003182, 40.492555539298088),\n",
    "    (\"Z2\", \"36L/18R\", -3.575903257941006, 40.491865496230126),\n",
    "    (\"Z3\", \"36L/18R\", -3.57319305240692, 40.491819096186241),\n",
    "    (\"LF\", \"32L/14R\", -3.572566658955927, 40.479721203031424),\n",
    "    (\"L1\", \"32L/14R\", -3.57652786733783, 40.483565816902733),\n",
    "    (\"LA\", \"32L/14R\", -3.577181028787666, 40.484251101106899),\n",
    "    (\"LB\", \"32L/14R\", -3.577553413710587, 40.484873329796898),\n",
    "    (\"LC\", \"32L/14R\", -3.575750378154376, 40.486690643924192),\n",
    "    (\"LD\", \"32L/14R\", -3.575150753600524, 40.486522892072891),\n",
    "    (\"LE\", \"32L/14R\", -3.574915186586964, 40.485580625293494)\n",
    "]\n",
    "# Esquema para la UDF (dos campos de tipo String: Designator y Runway)\n",
    "schema = StructType([\n",
    "    StructField(\"Designator\", StringType(), True),\n",
    "    StructField(\"Runway\", StringType(), True)\n",
    "])\n",
    "# Registrar la UDF con el tipo de retorno correcto (STRUCT)\n",
    "assign_udf = udf(lambda lat, lon: assign_designator_runway(lat, lon, holding_points), schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignHoldingPoint(df_pos_callsign):\n",
    "    # Asignamos puntos de espera\n",
    "    df_with_hp = df_pos_callsign.withColumn(\n",
    "        \"Designator_Runway\", \n",
    "        assign_udf(\"lat\", \"lon\")\n",
    "    )\n",
    "    \n",
    "    # Separamos en dos columnas el resultado\n",
    "    df_with_hp = df_with_hp.withColumn(\n",
    "        \"Designator\", F.col(\"Designator_Runway.Designator\")\n",
    "    ).withColumn(\n",
    "        \"Runway\", F.col(\"Designator_Runway.Runway\")\n",
    "    )\n",
    "    df_with_hp = df_with_hp.drop(\"Designator_Runway\")\n",
    "    \n",
    "    return df_with_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 6. Combinamos con las categorías de turbulencia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 7. Eliminamos todos los vuelos que no se les ha detectado en un punto de espera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "def filterFlights(df_with_hp_tc):\n",
    "    # Filtramos solo los vuelos que alguna vez tuvieron un Designator no nulo\n",
    "    df_holding_count = df_with_hp_tc.groupBy(\"Callsign\").agg(\n",
    "        count(when(col(\"Designator\").isNotNull(), True)).alias(\"holding_count\")\n",
    "    ).filter(col(\"holding_count\") > 0)\n",
    "    \n",
    "    # Nos quedamos solo con los vuelos válidos (que pasaron por un holding point)\n",
    "    df_valid_callsigns = df_holding_count.select(\"Callsign\")\n",
    "    \n",
    "    # Unimos para mantener solo las filas de esos vuelos\n",
    "    df_valid_flights = df_with_hp_tc.join(df_valid_callsigns, on=\"Callsign\", how=\"inner\")\n",
    "    \n",
    "    return df_valid_flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 8. Nos quedamos con las posiciones desde que se le detecta en un punto de espera hasta la primera vez que se le detecta en el aire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import min\n",
    "from pyspark.sql.functions import when, col\n",
    "def filterPositions(df_valid_flights):\n",
    "    # Ventana por Callsign\n",
    "    window_spec = Window.partitionBy(\"Callsign\")\n",
    "    \n",
    "    # Añadimos la primera vez que el vuelo fue detectado en tierra\n",
    "    df_with_first_on_ground = df_valid_flights.withColumn(\n",
    "        \"first_on_ground_time\",\n",
    "        min(when(col(\"Flight status\") == \"on-ground\", col(\"Timestamp\"))).over(window_spec)\n",
    "    )\n",
    "    \n",
    "    # Encontramos el primer timestamp en que el Designator no es nulo\n",
    "    df_with_first_holding = df_with_first_on_ground.withColumn(\n",
    "        \"first_holding_time\",\n",
    "        min(when(col(\"Designator\").isNotNull(), col(\"Timestamp\"))).over(window_spec)\n",
    "    )\n",
    "    \n",
    "    # Filtramos solo las filas en o después del primer punto de espera\n",
    "    df_from_hp = df_with_first_holding.filter(col(\"Timestamp\") >= col(\"first_holding_time\"))\n",
    "    \n",
    "    # Encontramos el primer timestamp en que FlightStatus es 'airborne'\n",
    "    df_with_first_airborne = df_from_hp.withColumn(\n",
    "        \"first_airborne_time\",\n",
    "        F.min(F.when(F.col(\"Flight status\") == \"airborne\", F.col(\"Timestamp\"))).over(window_spec)\n",
    "    )\n",
    "    \n",
    "    # Filtramos solo las filas en o después del primer estado airborne\n",
    "    df_takeoff_segment = df_with_first_airborne.filter(F.col(\"Timestamp\") <= F.col(\"first_airborne_time\"))\n",
    "    \n",
    "    return df_takeoff_segment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 9. Combinamos con los mensajes de velocidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge de posiciones y velocidades\n",
    "def mergePositionsVelocities(df_takeoff_segment, df_speed):\n",
    "    TOLERANCE_VELOCITY_ASSIGNMENT = 1  # 1 segundo\n",
    "    \n",
    "    df_takeoff_segment = df_takeoff_segment.withColumn(\"Timestamp_sec\", F.unix_timestamp(\"Timestamp\"))\n",
    "    df_speed = df_speed.withColumn(\"Timestamp_sec\", F.unix_timestamp(\"Timestamp (date)\"))\n",
    "    \n",
    "    df_speed = df_speed.withColumnRenamed(\"Timestamp_sec\", \"Timestamp_sec_speed\") \\\n",
    "                           .withColumnRenamed(\"Timestamp (date)\", \"Timestamp_speed\") \\\n",
    "                           .withColumnRenamed(\"Flight status\", \"Flight status Speed\")\n",
    "    \n",
    "    df_with_velocities = df_takeoff_segment.join(df_speed, on=\"ICAO\", how=\"inner\").filter(\n",
    "        F.abs(df_takeoff_segment[\"Timestamp_sec\"] - df_speed[\"Timestamp_sec_speed\"]) <= TOLERANCE_VELOCITY_ASSIGNMENT\n",
    "    )\n",
    "    \n",
    "    df_with_velocities = df_with_velocities.drop(\"Timestamp_sec\", \"Timestamp_sec_speed\", \"Timestamp_speed\", \"Flight status Speed\")\n",
    "    df_with_velocities = df_with_velocities.dropDuplicates([\"ICAO\", \"Timestamp\"])\n",
    "    \n",
    "    return df_with_velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 10. Filtramos para quedarnos solo con las aeronaves que en algún momento se paran en un punto de espera y recalculamos primer timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importantTakeoffs(df_with_velocities):\n",
    "    # Filtrar los callsigns con velocidad 0 y punto de espera no nulo\n",
    "    df_callsigns_zero_speed = df_with_velocities.filter(\n",
    "        (col(\"Speed\") == 0) & \n",
    "        (col(\"Designator\").isNotNull())\n",
    "    ).select(\"ICAO\").distinct()\n",
    "    \n",
    "    # Nos quedamos solo con estos vuelos\n",
    "    df_important_takeoffs = df_with_velocities.join(\n",
    "        df_callsigns_zero_speed,\n",
    "        on=\"ICAO\",\n",
    "        how=\"inner\" \n",
    "    )\n",
    "    \n",
    "    # Ventana por Callsign\n",
    "    window_spec = Window.partitionBy(\"Callsign\")\n",
    "    \n",
    "    # Encontramos el primer timestamp en que el Designator no es nulo\n",
    "    df_important_takeoffs = df_important_takeoffs.withColumn(\n",
    "        \"first_holding_time\",\n",
    "        min(when(col(\"Designator\").isNotNull() & (F.col(\"Speed\") == 0), col(\"Timestamp\"))).over(window_spec)\n",
    "    )\n",
    "    \n",
    "    # Filtramos solo las filas en o después del primer punto de espera\n",
    "    df_important_takeoffs = df_important_takeoffs.filter(col(\"Timestamp\") >= col(\"first_holding_time\"))\n",
    "    \n",
    "    return df_important_takeoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 11. Calculamos tiempos de espera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Agrupar por identificador de vuelo\n",
    "df_grouped = df_important_takeoffs.groupBy(\"Callsign\") \\\n",
    "                .agg(\n",
    "                    F.first(\"Designator\", ignorenulls=True).alias(\"Designator\"),\n",
    "                    F.first(\"Runway\", ignorenulls=True).alias(\"Runway\"),\n",
    "                    F.first(\"ICAO\", ignorenulls=True).alias(\"ICAO\"),\n",
    "                    F.first(\"lat\", ignorenulls=True).alias(\"lat\"),\n",
    "                    F.first(\"lon\", ignorenulls=True).alias(\"lon\"),\n",
    "                    F.first(\"TurbulenceCategory\", ignorenulls=True).alias(\"TurbulenceCategory\"),\n",
    "                    F.first(\"first_holding_time\").alias(\"first_holding_time\"),\n",
    "                    F.first(\"first_airborne_time\").alias(\"first_airborne_time\"),\n",
    "                    F.first(\"first_on_ground_time\").alias(\"first_on_ground_time\")\n",
    "                )\n",
    "# Calculamos los tiempos de espera\n",
    "dfB = df_grouped.withColumn(\"takeoff time\", \n",
    "                   (F.unix_timestamp(\"first_airborne_time\") - F.unix_timestamp(\"first_holding_time\"))\n",
    "                   .cast(\"double\"))\n",
    "                   \n",
    "dfB = dfB.withColumn(\"time_to_holding_point\", \n",
    "                   (F.unix_timestamp(\"first_holding_time\") - F.unix_timestamp(\"first_on_ground_time\"))\n",
    "                   .cast(\"double\"))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateHoldingTime(df_important_takeoffs):\n",
    "    df_holding = df_important_takeoffs.filter(\n",
    "        (col(\"Speed\") == 0) & \n",
    "        (col(\"Designator\").isNotNull())\n",
    "    )\n",
    "    \n",
    "    dfB = df_holding.withColumn(\"takeoff time\", \n",
    "                       (F.unix_timestamp(\"first_airborne_time\") - F.unix_timestamp(\"Timestamp\"))\n",
    "                       .cast(\"double\"))\n",
    "                       \n",
    "    dfB = dfB.withColumn(\"time_before_holding_point\", \n",
    "                       (F.unix_timestamp(\"first_holding_time\") - F.unix_timestamp(\"first_on_ground_time\"))\n",
    "                       .cast(\"double\"))\n",
    "                       \n",
    "    dfB = dfB.withColumn(\"time_at_holding_point\", \n",
    "                       (F.unix_timestamp(\"Timestamp\") - F.unix_timestamp(\"first_holding_time\"))\n",
    "                       .cast(\"double\"))\n",
    "    \n",
    "    return dfB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 12. Con dataframe A, creamos otro dataframe que indique cada 10s qué pistas y puntos de espera están ocupados → dataframe C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BooleanType, StringType\n",
    "RUNWAYS = [\n",
    "    { \"type\": \"Feature\", \"properties\": { \"Runway\": \"36R/18L\" }, \"geometry\": { \"type\": \"Polygon\", \"coordinates\": [ [ [ -3.560314698993372, 40.536214953564219 ], [ -3.560115619892884, 40.499859371026062 ], [ -3.558758924220036, 40.499866800139706 ], [ -3.558958003320524, 40.536222382677863 ], [ -3.560314698993372, 40.536214953564219 ] ] ] } },\n",
    "    { \"type\": \"Feature\", \"properties\": { \"Runway\": \"32R/14L\" }, \"geometry\": { \"type\": \"Polygon\", \"coordinates\": [ [ [ -3.560616443164626, 40.49648882920043 ], [ -3.530233771171787, 40.466560555412769 ], [ -3.529390530746063, 40.467416598679677 ], [ -3.559773202738903, 40.49734487246733 ], [ -3.560616443164626, 40.49648882920043 ] ] ] } },\n",
    "    { \"type\": \"Feature\", \"properties\": { \"Runway\": \"36L/18R\" }, \"geometry\": { \"type\": \"Polygon\", \"coordinates\": [ [ [ -3.575312220900675, 40.533035679421005 ], [ -3.575065318943644, 40.492052937902109 ], [ -3.574163865545885, 40.492058368739578 ], [ -3.574410767502916, 40.533041110258473 ], [ -3.575312220900675, 40.533035679421005 ] ] ] } },\n",
    "    { \"type\": \"Feature\", \"properties\": { \"Runway\": \"32L/14R\" }, \"geometry\": { \"type\": \"Polygon\", \"coordinates\": [ [ [ -3.545264836113798, 40.455701584425526 ], [ -3.576990231488946, 40.486626436878893 ], [ -3.577924321488386, 40.485668166355438 ], [ -3.546198926113238, 40.454743313902071 ], [ -3.545264836113798, 40.455701584425526 ] ] ] } }\n",
    "]\n",
    "RUNWAY_POLYGONS = {\n",
    "    r[\"properties\"][\"Runway\"]: r[\"geometry\"][\"coordinates\"][0]\n",
    "    for r in RUNWAYS\n",
    "}\n",
    "RUNWAY_NAMES = [\n",
    "    r[\"properties\"][\"Runway\"]\n",
    "    for r in RUNWAYS\n",
    "]\n",
    "HP_NAMES = [\n",
    "    hp[0]\n",
    "    for hp in holding_points\n",
    "]\n",
    "def point_in_polygon(lat, lon, polygon):\n",
    "    \"\"\"\n",
    "    Algoritmo que calcula si un punto está dentro de un polígono.\n",
    "    \n",
    "    lat, lon: coordenadas del punto\n",
    "    polygon: lista de coordenadas [(lon1, lat1), (lon2, lat2), ...] del polígono (ojo al orden lon/lat)\n",
    "    \"\"\"\n",
    "    num = len(polygon)\n",
    "    j = num - 1\n",
    "    inside = False\n",
    "    for i in range(num):\n",
    "        xi, yi = polygon[i]\n",
    "        xj, yj = polygon[j]\n",
    "        if ((yi > lat) != (yj > lat)) and \\\n",
    "           (lon < (xj - xi) * (lat - yi) / (yj - yi + 1e-9) + xi):\n",
    "            inside = not inside\n",
    "        j = i\n",
    "    return inside\n",
    "    \n",
    "# Obtenemos las pistas ocupadas (geométricamente)\n",
    "def get_runway_for_point(lat, lon):\n",
    "    for runway, polygon in RUNWAY_POLYGONS.items():\n",
    "        if point_in_polygon(lat, lon, polygon):\n",
    "            return runway\n",
    "    return None  # si no está en ninguna pista\n",
    "get_runway_udf = udf(lambda lat, lon: get_runway_for_point(lat, lon), StringType())\n",
    "def occupaidEach10s(dfA):\n",
    "    # Redondear el timestamp a bloques de 10 segundos\n",
    "    # La columna time_10s va a contener timestamps redondeados a bloques de 10 segundos en formato UNIX\n",
    "    dfA = dfA.withColumn(\"time_10s\", (F.ceil(F.col(\"Timestamp\").cast(\"long\") / 10).cast(\"long\") * 10))\n",
    "    dfA = dfA.withColumn(\"time_10s\", F.from_unixtime(F.col(\"time_10s\")))\n",
    "    \n",
    "    # Obtenemos los puntos de espera ocupados en cada intervalo\n",
    "    # collect_set(): Agrupa todos los valores únicos de una columna (en este caso \"Designator\") dentro de cada grupo definido por un groupBy() o window y devuelve una lista sin duplicados\n",
    "    dfA_holding_points = dfA.groupBy(\"time_10s\")\\\n",
    "        .agg(F.collect_set(\"Designator\").alias(\"occupied_holding_points\"))\n",
    "    \n",
    "    dfA = dfA.withColumn(\"RunwayFromPosition\", get_runway_udf(\"lat\", \"lon\"))\n",
    "    # Pistas ocupadas por cada intervalo\n",
    "    dfA_runways = dfA.groupBy(\"time_10s\")\\\n",
    "        .agg(F.collect_set(\"RunwayFromPosition\").alias(\"occupied_runways\"))\n",
    "    \n",
    "    status_by_interval = dfA_holding_points.join(dfA_runways, on=\"time_10s\")\n",
    "    \n",
    "    # Generamos las columnas booleanas para puntos de espera\n",
    "    for hp in HP_NAMES:\n",
    "        status_by_interval = status_by_interval.withColumn(f\"{hp}\", F.array_contains(F.col(\"occupied_holding_points\"), hp))\n",
    "        \n",
    "    # Y para las pistas\n",
    "    for rw in RUNWAY_NAMES:\n",
    "        safe_rw = rw.replace(\"/\", \"_\")  # Evita caracteres problemáticos en nombres de columna\n",
    "        status_by_interval = status_by_interval.withColumn(f\"{safe_rw}\", F.array_contains(F.col(\"occupied_runways\"), rw))\n",
    "    \n",
    "    status_by_interval_final = status_by_interval.drop(\"occupied_holding_points\", \"occupied_runways\")\n",
    "    return status_by_interval_final, dfA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 13. Con dataframe A, creamos otro dataframe que indique todos los despegues / aterrizajes por pista, debe incluir el timestamp y la categoría de turbulencia del avión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "def eventsByRunway(dfA):\n",
    "    \n",
    "    window_spec = Window.partitionBy(\"ICAO\").orderBy(\"Timestamp\")\n",
    "    \n",
    "    dfA = dfA.withColumn(\"Runway\", F.coalesce(\"Runway\", \"RunwayFromPosition\"))\n",
    "    dfA = dfA.withColumn(\"prev_status\", F.lag(\"Flight status\").over(window_spec))\n",
    "    \n",
    "    dfD = dfA.filter(\n",
    "        ((F.col(\"Flight status\") == \"airborne\") & (F.col(\"prev_status\") == \"on-ground\")) |\n",
    "        ((F.col(\"Flight status\") == \"on-ground\") & (F.col(\"prev_status\") == \"airborne\"))\n",
    "    ).withColumn(\n",
    "        \"Event\",\n",
    "        F.when(F.col(\"Flight status\") == \"airborne\", F.lit(\"takeoff\"))\n",
    "         .when(F.col(\"Flight status\") == \"on-ground\", F.lit(\"landing\"))\n",
    "    )\n",
    "    \n",
    "    \n",
    "    dfD = dfD.select(\"Runway\", \"Timestamp\", \"TurbulenceCategory\", \"Event\")\n",
    "    \n",
    "    dfD = dfD.filter(F.col(\"Runway\").isNotNull())\n",
    "    \n",
    "    return dfD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 14. Agrupamos D por minuto para conseguir tasa de despegues y de aterrizajes por minuto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, when\n",
    "def eventsMinuteRate(dfD):\n",
    "    dfE = dfD.withColumn(\n",
    "        \"Minute\", \n",
    "        F.from_unixtime(\n",
    "            F.ceil(F.unix_timestamp(F.col(\"Timestamp\")) / 60) * 60\n",
    "        ).cast(\"timestamp\")\n",
    "    ) \\\n",
    "    .groupBy(\"Minute\") \\\n",
    "    .agg(\n",
    "        count(when(F.col(\"Event\") == \"takeoff\", True)).alias(\"last min takeoffs\"),\n",
    "        count(when(F.col(\"Event\") == \"landing\", True)).alias(\"last min landings\")\n",
    "    )\n",
    "    \n",
    "    return dfE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 15. Combinamos B con C, D y E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import col, unix_timestamp, from_unixtime, round\n",
    "from pyspark.sql.functions import date_trunc\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, last\n",
    "from pyspark.sql.functions import row_number\n",
    "def combineBCDE(dfB, dfC, dfD, dfE):\n",
    "    # Redondeamos a múltiplos de 10 segundos\n",
    "    dfB = dfB.withColumn(\n",
    "        \"time_10s\",\n",
    "        from_unixtime((unix_timestamp(\"Timestamp\") / 10).cast(\"long\") * 10)\n",
    "    )\n",
    "    \n",
    "    # Paso 2: Join con dfC\n",
    "    dfF = dfB.join(dfC, on=\"time_10s\", how=\"inner\")\n",
    "    \n",
    "    # Paso 1: Redondear a minuto\n",
    "    dfF = dfF.withColumn(\n",
    "        \"Minute\",\n",
    "        date_trunc(\"minute\", col(\"Timestamp\"))\n",
    "    )\n",
    "    \n",
    "    # Paso 2: Join con dfE\n",
    "    dfF = dfF.join(dfE, on=\"Minute\", how=\"inner\")\n",
    "    \n",
    "    # Paso 1: Creamos la window\n",
    "    w = Window.partitionBy(\"Runway\").orderBy(\"Timestamp\").rangeBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "    \n",
    "    # Paso 2: Renombramos Timestamp para evitar conflictos en el join\n",
    "    dfD = dfD.withColumnRenamed(\"Timestamp\", \"event_timestamp\")\n",
    "    dfD = dfD.withColumnRenamed(\"TurbulenceCategory\", \"last_event_turb_cat\")\n",
    "    dfD = dfD.withColumnRenamed(\"Event\", \"last_event\")\n",
    "    \n",
    "    # Paso 3: Join de los dataframes en base a Runway y evento previo en el tiempo\n",
    "    df_combined = dfF.join(dfD, on=\"Runway\", how=\"left\") \\\n",
    "        .filter(col(\"event_timestamp\") < col(\"Timestamp\"))\n",
    "    \n",
    "    # Paso 4: Usamos Window para quedarnos con la fila más reciente antes de `first_holding_time`\n",
    "    w2 = Window.partitionBy(\"Callsign\", \"Timestamp\", \"Runway\").orderBy(col(\"event_timestamp\").desc())\n",
    "    \n",
    "    df_final = df_combined.withColumn(\"rank\", row_number().over(w2)) \\\n",
    "        .filter(col(\"rank\") == 1) \\\n",
    "        .drop(\"rank\")\n",
    "    \n",
    "    df_final = df_final.withColumn(\n",
    "        \"time_since_last_event_seconds\",\n",
    "        unix_timestamp(\"Timestamp\") - unix_timestamp(\"event_timestamp\")\n",
    "    )\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 16. Extraemos la hora, día de la semana y si es festivo o no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hora, dia de la semana y si es festivo o no\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BooleanType\n",
    "import json\n",
    "from pyspark.sql.functions import to_date, hour, date_format, udf\n",
    "from pyspark.sql.types import BooleanType\n",
    "# Lista en json de festivos en España y en la Comunidad de Madrid extraída en local (no está instalada la librería 'holidays' en Zepelin)\n",
    "holiday_list_json = [\n",
    "  {\n",
    "    \"date\": \"2024-01-01\",\n",
    "    \"name\": \"A\\u00f1o Nuevo\"\n",
    "  },\n",
    "  {\n",
    "    \"date\": \"2024-01-06\",\n",
    "    \"name\": \"Epifan\\u00eda del Se\\u00f1or\"\n",
    "  },\n",
    "  {\n",
    "    \"date\": \"2024-03-29\",\n",
    "    \"name\": \"Viernes Santo\"\n",
    "  },\n",
    "  {\n",
    "    \"date\": \"2024-05-01\",\n",
    "    \"name\": \"Fiesta del Trabajo\"\n",
    "  },\n",
    "  {\n",
    "    \"date\": \"2024-08-15\",\n",
    "    \"name\": \"Asunci\\u00f3n de la Virgen\"\n",
    "  },\n",
    "  {\n",
    "    \"date\": \"2024-10-12\",\n",
    "    \"name\": \"Fiesta Nacional de Espa\\u00f1a\"\n",
    "  },\n",
    "  {\n",
    "    \"date\": \"2024-11-01\",\n",
    "    \"name\": \"Todos los Santos\"\n",
    "  },\n",
    "  {\n",
    "    \"date\": \"2024-12-06\",\n",
    "    \"name\": \"D\\u00eda de la Constituci\\u00f3n Espa\\u00f1ola\"\n",
    "  },\n",
    "  {\n",
    "    \"date\": \"2024-12-25\",\n",
    "    \"name\": \"Natividad del Se\\u00f1or\"\n",
    "  },\n",
    "  {\n",
    "    \"date\": \"2024-03-28\",\n",
    "    \"name\": \"Jueves Santo\"\n",
    "  },\n",
    "  {\n",
    "    \"date\": \"2024-05-02\",\n",
    "    \"name\": \"Fiesta de la Comunidad de Madrid\"\n",
    "  },\n",
    "  {\n",
    "    \"date\": \"2024-07-25\",\n",
    "    \"name\": \"Santiago Ap\\u00f3stol\"\n",
    "  },\n",
    "  {\n",
    "    \"date\": \"2025-01-01\",\n",
    "    \"name\": \"A\\u00f1o Nuevo\"\n",
    "  },\n",
    "  {\n",
    "    \"date\": \"2025-01-06\",\n",
    "    \"name\": \"Epifan\\u00eda del Se\\u00f1or\"\n",
    "  },\n",
    "  {\n",
    "    \"date\": \"2025-04-18\",\n",
    "    \"name\": \"Viernes Santo\"\n",
    "  },\n",
    "  {\n",
    "    \"date\": \"2025-05-01\",\n",
    "    \"name\": \"Fiesta del Trabajo\"\n",
    "  },\n",
    "  {\n",
    "    \"date\": \"2025-08-15\",\n",
    "    \"name\": \"Asunci\\u00f3n de la Virgen\"\n",
    "  },\n",
    "  {\n",
    "    \"date\": \"2025-11-01\",\n",
    "    \"name\": \"Todos los Santos\"\n",
    "  },\n",
    "  {\n",
    "    \"date\": \"2025-12-06\",\n",
    "    \"name\": \"D\\u00eda de la Constituci\\u00f3n Espa\\u00f1ola\"\n",
    "  },\n",
    "  {\n",
    "    \"date\": \"2025-12-08\",\n",
    "    \"name\": \"Inmaculada Concepci\\u00f3n\"\n",
    "  },\n",
    "  {\n",
    "    \"date\": \"2025-12-25\",\n",
    "    \"name\": \"Natividad del Se\\u00f1or\"\n",
    "  },\n",
    "  {\n",
    "    \"date\": \"2025-04-17\",\n",
    "    \"name\": \"Jueves Santo\"\n",
    "  },\n",
    "  {\n",
    "    \"date\": \"2025-05-02\",\n",
    "    \"name\": \"Fiesta de la Comunidad de Madrid\"\n",
    "  },\n",
    "  {\n",
    "    \"date\": \"2025-07-25\",\n",
    "    \"name\": \"Santiago Ap\\u00f3stol\"\n",
    "  }\n",
    "]\n",
    "# Crear conjunto de fechas festivas (como strings \"yyyy-MM-dd\")\n",
    "holiday_dates = set(item[\"date\"] for item in holiday_list_json)\n",
    "# UDF para marcar si una fecha es festiva\n",
    "is_holiday_udf = udf(lambda d: d.strftime(\"%Y-%m-%d\") in holiday_dates, BooleanType())\n",
    "def dateColumns(df_final):\n",
    "    # Añadir columnas a dfE\n",
    "    df_final = df_final.withColumn(\"Hour\", hour(\"Minute\")) \\\n",
    "             .withColumn(\"Weekday\", date_format(\"Minute\", \"E\")) \\\n",
    "             .withColumn(\"Date\", to_date(\"Minute\")) \\\n",
    "             .withColumn(\"IsHoliday\", is_holiday_udf(\"Date\")) \\\n",
    "             .drop(\"Date\")\n",
    "             \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 17. Extraemos la aerolínea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%md\n",
    "#### 18. Renombramos y reordenamos columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "def cleanColumns(df_final):\n",
    "    # Suponiendo que df es tu DataFrame original\n",
    "    df_final_clean = df_final.select(\n",
    "        F.col('takeoff time').alias('takeoff_time'),\n",
    "        F.col('Timestamp').alias('timestamp'),\n",
    "        F.col('ICAO').alias('icao'),\n",
    "        F.col('Callsign').alias('callsign'),\n",
    "        F.col('Designator').alias('holding_point'),\n",
    "        F.col('Runway').alias('runway'),\n",
    "        F.col('operator'),\n",
    "        F.col('TurbulenceCategory').alias('turbulence_category'),\n",
    "        F.col('lat'),\n",
    "        F.col('lon'),\n",
    "        F.col('last min takeoffs').alias('last_min_takeoffs'),\n",
    "        F.col('last_event'),\n",
    "        F.col('last min landings').alias('last_min_landings'),\n",
    "        F.col('last_event_turb_cat').alias('last_event_turb_cat'),\n",
    "        F.col('time_since_last_event_seconds'),\n",
    "        F.col('time_before_holding_point'),\n",
    "        F.col('time_at_holding_point'),\n",
    "        F.col('Hour').alias('hour'),\n",
    "        F.col('Weekday').alias('weekday'),\n",
    "        F.col('IsHoliday').alias('is_holiday'),\n",
    "        F.col('event_timestamp'),\n",
    "        F.col('first_holding_time'),\n",
    "        F.col('first_airborne_time'),\n",
    "        F.col('first_on_ground_time'),\n",
    "        'Z1', 'KA6', 'KA8', 'K3', 'K2', 'K1', 'Y1', 'Y2', 'Y3', 'Y7', 'Z6', 'Z4', 'Z2', \n",
    "        'Z3', 'LF', 'L1', 'LA', 'LB', 'LC', 'LD', 'LE', '36R_18L', '32R_14L', '36L_18R', '32L_14R'\n",
    "        \n",
    "    )\n",
    "    \n",
    "    return df_final_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Definir las rutas de los archivos de entrada y salida\n",
    "input_path = \"/data/proyecto2/outputs/iberIA/decoded\"\n",
    "output_path = \"/data/proyecto2/outputs/iberIA/processed\"\n",
    "# --- PROCESAR TODOS LOS QUE ESTAN EN LA CARPETA DECODED ---\n",
    "\"\"\"\n",
    "# Obtener el sistema de archivos\n",
    "fs = spark.sparkContext._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "# Listar los archivos en el directorio especificado\n",
    "files = fs.listStatus(spark._jvm.org.apache.hadoop.fs.Path(input_path))\n",
    "# Extraer solo los nombres de archivo\n",
    "file_names = [file.getPath().getName() for file in files]\n",
    "file_names\n",
    "\"\"\"\n",
    "# --- PROCESAR UNA LISTA DE NOMBRES DE FICHEROS ---\n",
    "file_names = [\n",
    "    \"dia_28_01_2025.parquet\",\n",
    "    \"dia_29_01_2025.parquet\",\n",
    "    \"dia_30_01_2025.parquet\",\n",
    "    \"dia_31_01_2025.parquet\",\n",
    "]\n",
    "# Bucle para procesar cada archivo\n",
    "for file in file_names:\n",
    "    # Extraer la parte de la fecha del nombre del archivo\n",
    "    date_str = file.replace(\"dia_\", \"\").replace(\".parquet\", \"\")\n",
    "    day, month, year = date_str.split(\"_\")\n",
    "    output_file = f\"{year}_{month}_{day}\"\n",
    "    \n",
    "    # Definir la ruta del archivo de salida\n",
    "    output_file_path = os.path.join(output_path, output_file)\n",
    "    # 1. Leemos el archivo parquet\n",
    "    df_raw = spark.read.parquet(os.path.join(input_path, file))\n",
    "    # 2.\n",
    "    # posiciones\n",
    "    df_pos = getPositions(df_raw)\n",
    "    \n",
    "    # vuelos\n",
    "    df_flights = getFlights(df_raw)\n",
    "    \n",
    "    # categorías de turbulencia\n",
    "    df_types = getAirplaneCategories(df_raw)\n",
    "    \n",
    "    # velocidades\n",
    "    df_speed = getVelocities(df_raw)\n",
    "    \n",
    "    # altitudes\n",
    "    df_alt = getAltitudes(df_raw)\n",
    "    \n",
    "    # 3.\n",
    "    df_pos_airport = filter_positions_within_radius(df_pos)\n",
    "    \n",
    "    # 4.\n",
    "    df_pos_callsign = combinePsitionsFlights(df_pos_airport, df_flights)\n",
    "    \n",
    "    # 5.\n",
    "    df_with_hp = assignHoldingPoint(df_pos_callsign)\n",
    "    \n",
    "    # 6.\n",
    "    df_with_hp_tc = df_with_hp.join(df_types, on=\"ICAO\", how=\"inner\")\n",
    "    dfA = df_with_hp_tc\n",
    "    \n",
    "    # 7.\n",
    "    df_valid_flights = filterFlights(df_with_hp_tc)\n",
    "    \n",
    "    # 8.\n",
    "    df_takeoff_segment = filterPositions(df_valid_flights)\n",
    "    \n",
    "    # 9.\n",
    "    df_with_velocities = mergePositionsVelocities(df_takeoff_segment, df_speed)\n",
    "    \n",
    "    # 10. \n",
    "    df_important_takeoffs = importantTakeoffs(df_with_velocities)\n",
    "    \n",
    "    # 11.\n",
    "    dfB = calculateHoldingTime(df_important_takeoffs)\n",
    "    \n",
    "    # 12.\n",
    "    status_by_interval_final, dfA = occupaidEach10s(dfA)\n",
    "    dfC = status_by_interval_final\n",
    "    \n",
    "    # 13.\n",
    "    dfD = eventsByRunway(dfA)\n",
    "    \n",
    "    # 14.\n",
    "    dfE = eventsMinuteRate(dfD)\n",
    "    \n",
    "    # 15.\n",
    "    dfA.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    dfB.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    dfC.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    dfD.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    dfE.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    df_final = combineBCDE(dfB, dfC, dfD, dfE)\n",
    "    \n",
    "    # 16.\n",
    "    df_final = dateColumns(df_final)\n",
    "    \n",
    "    # 17.\n",
    "    df_final = df_final.withColumn(\"operator\", substring(\"Callsign\", 1, 3))\n",
    "    \n",
    "    # 18.\n",
    "    df_final_clean = cleanColumns(df_final)\n",
    "    \n",
    "    # Guardamos el archivo procesado en el directorio de salida\n",
    "    df_final_clean.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "    df_final_clean.show()\n",
    "    df_final_clean.coalesce(1).write.parquet(output_file_path)\n",
    "    print(f\"Archivo procesado: {file} -> {output_file}\")\n",
    "    \n",
    "    # Liberamos memoria\n",
    "    dfA.unpersist()\n",
    "    dfB.unpersist()\n",
    "    dfC.unpersist()\n",
    "    dfD.unpersist()\n",
    "    dfE.unpersist()\n",
    "    df_final_clean.unpersist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
