{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4533c4e",
   "metadata": {},
   "source": [
    "# Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466bc228",
   "metadata": {},
   "source": [
    "Procesamos los escenarios de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cee6efee-a05d-4cf6-9bee-9a6afe81e8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 23:22:49.905514: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-07 23:22:49.918253: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-07 23:22:49.933761: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-07 23:22:49.938177: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-07 23:22:49.949891: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-07 23:22:50.768777: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurando Spark Session...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/07 23:22:52 WARN Utils: Your hostname, bryanSpace resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/05/07 23:22:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario_name</th>\n",
       "      <th>icao</th>\n",
       "      <th>holding_point</th>\n",
       "      <th>runway</th>\n",
       "      <th>time_to_takeoff_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scenario_001.parquet</td>\n",
       "      <td>34324e</td>\n",
       "      <td>K2</td>\n",
       "      <td>14L/32R</td>\n",
       "      <td>-9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scenario_002.parquet</td>\n",
       "      <td>4bb26b</td>\n",
       "      <td>K1</td>\n",
       "      <td>14L/32R</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scenario_003.parquet</td>\n",
       "      <td>3451d8</td>\n",
       "      <td>K3</td>\n",
       "      <td>14L/32R</td>\n",
       "      <td>-9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scenario_004.parquet</td>\n",
       "      <td>3420ca</td>\n",
       "      <td>K2</td>\n",
       "      <td>14L/32R</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>scenario_005.parquet</td>\n",
       "      <td>344416</td>\n",
       "      <td>Z4</td>\n",
       "      <td>18R/36L</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>scenario_189.parquet</td>\n",
       "      <td>3445d4</td>\n",
       "      <td>Z4</td>\n",
       "      <td>18R/36L</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>scenario_190.parquet</td>\n",
       "      <td>4d251c</td>\n",
       "      <td>Z2</td>\n",
       "      <td>18R/36L</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>scenario_191.parquet</td>\n",
       "      <td>34628d</td>\n",
       "      <td>Z1</td>\n",
       "      <td>18R/36L</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>scenario_192.parquet</td>\n",
       "      <td>342107</td>\n",
       "      <td>Z4</td>\n",
       "      <td>18R/36L</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>scenario_193.parquet</td>\n",
       "      <td>344416</td>\n",
       "      <td>K2</td>\n",
       "      <td>14L/32R</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>193 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            scenario_name    icao holding_point   runway  time_to_takeoff_s\n",
       "0    scenario_001.parquet  34324e            K2  14L/32R               -9.0\n",
       "1    scenario_002.parquet  4bb26b            K1  14L/32R               -5.0\n",
       "2    scenario_003.parquet  3451d8            K3  14L/32R               -9.0\n",
       "3    scenario_004.parquet  3420ca            K2  14L/32R                NaN\n",
       "4    scenario_005.parquet  344416            Z4  18R/36L                NaN\n",
       "..                    ...     ...           ...      ...                ...\n",
       "188  scenario_189.parquet  3445d4            Z4  18R/36L                NaN\n",
       "189  scenario_190.parquet  4d251c            Z2  18R/36L                NaN\n",
       "190  scenario_191.parquet  34628d            Z1  18R/36L                NaN\n",
       "191  scenario_192.parquet  342107            Z4  18R/36L                NaN\n",
       "192  scenario_193.parquet  344416            K2  14L/32R                NaN\n",
       "\n",
       "[193 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from preprocess import pipeline, meteo, depurador\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "RESULT_PATH = \"data/final_scenarios/answers_empty.csv\"\n",
    "MODEL_PATH = \"models/modeloTransformer3.keras\"\n",
    "METEO_PATH = \"dato/completo/open-meteo-40.53N3.56W602m-2.csv\"\n",
    "DIR = \"data/final_scenarios/clean\"\n",
    "\n",
    "# Inicializamos la sesión de Spark\n",
    "print(\"Configurando Spark Session...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PrediccionTiempoEsperaAvionNotebook\") \\\n",
    "    .config(\"spark.master\", \"local[*]\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"file:///\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", f\"file:///{os.path.abspath('spark-warehouse')}\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home={os.path.abspath('derby_metastore_db')}\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Cargamos el df de resultados\n",
    "df_results = pd.read_csv(RESULT_PATH)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b60b38b0-7823-46c6-aea9-8c6cac6ac66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el pipeline de preprocesado\n",
    "pipeline = pipeline.Pipeline(spark_session=spark)\n",
    "\n",
    "# Creamos el depurador\n",
    "depurador = depurador.Depurador()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88653f08-7a26-4bde-a6f8-2caba4a3db82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_meteo_data(spark_df, meteo_csv_path):\n",
    "    \"\"\"Carga datos meteorológicos y los une a un DataFrame Spark sin prefijos innecesarios.\"\"\"\n",
    "    print(f\"Cargando datos meteorológicos desde: {meteo_csv_path}\")\n",
    "\n",
    "    # Leer CSV con Pandas y eliminar columnas duplicadas si las hay\n",
    "    pdf_meteo = pd.read_csv(meteo_csv_path, skiprows=2)\n",
    "    pdf_meteo = pdf_meteo.loc[:, ~pdf_meteo.columns.duplicated()]\n",
    "    \n",
    "    df_meteo = spark.createDataFrame(pdf_meteo)\n",
    "\n",
    "    # Convertir columna 'time' en timestamp\n",
    "    if 'time' in df_meteo.columns:\n",
    "        df_meteo = df_meteo.withColumn(\"time_hour_meteo\", to_timestamp(col(\"time\")))\n",
    "        df_meteo = df_meteo.drop(\"time\")\n",
    "    else:\n",
    "        print(\"Error: Columna 'time' no encontrada en DataFrame meteo.\")\n",
    "        return spark_df\n",
    "\n",
    "    if 'timestamp' in spark_df.columns:\n",
    "        spark_df = spark_df.withColumn(\"time_hour\", date_trunc(\"hour\", col(\"timestamp\")))\n",
    "    else:\n",
    "        print(\"Error: Columna 'timestamp' no encontrada en DataFrame principal.\")\n",
    "        return spark_df\n",
    "\n",
    "    # --- ELIMINAR columnas duplicadas antes del join ---\n",
    "    meteo_cols = set(df_meteo.columns) - {\"time_hour_meteo\"}\n",
    "    spark_cols = set(spark_df.columns)\n",
    "    duplicated_cols = list(meteo_cols & spark_cols)\n",
    "    if duplicated_cols:\n",
    "        print(f\"Eliminando columnas duplicadas del df principal antes del join: {duplicated_cols}\")\n",
    "        spark_df = spark_df.drop(*duplicated_cols)\n",
    "\n",
    "    # Realizar el join\n",
    "    print(\"Uniendo datos principales con datos meteorológicos (join)...\")\n",
    "    joined_df = spark_df.join(df_meteo, spark_df[\"time_hour\"] == df_meteo[\"time_hour_meteo\"], how=\"inner\")\n",
    "\n",
    "    # Eliminar columnas auxiliares\n",
    "    joined_df = joined_df.drop(\"time_hour\", \"time_hour_meteo\")\n",
    "    print(\"Unión completada sin prefijos.\")\n",
    "    return joined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c59bb26-4e85-4f4e-a5a5-f31b1e674ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intentando cargar Pipeline Spark ML ajustado desde: ./models/completo/fitted_spark_ml_pipeline_model3/\n",
      "Pipeline Spark ML ajustado cargado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Importa PipelineModel, no Pipeline, para cargar un modelo ajustado\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql import functions as F\n",
    "from sklearn.preprocessing import MinMaxScaler as SklearnMinMaxScaler # Importar Sklearn MinMaxScaler\n",
    "from pyspark.sql.functions import sin, cos, pi, col, lit, when, lower, to_timestamp, date_trunc, row_number, monotonically_increasing_id, log1p\n",
    "\n",
    "# --- Define la ruta donde guardaste tu pipeline Spark ML ajustado ---\n",
    "# Basado en tu salida, parece que esta ruta es correcta ahora.\n",
    "FITTED_SPARK_PIPELINE_PATH = \"./models/completo/fitted_spark_ml_pipeline_model3/\"\n",
    "\n",
    "print(f\"Intentando cargar Pipeline Spark ML ajustado desde: {FITTED_SPARK_PIPELINE_PATH}\")\n",
    "\n",
    "try:\n",
    "    # Usa PipelineModel.load() para cargar un modelo ajustado\n",
    "    fitted_spark_pipeline = PipelineModel.load(FITTED_SPARK_PIPELINE_PATH)\n",
    "    print(\"Pipeline Spark ML ajustado cargado exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar el pipeline Spark ML ajustado desde '{FITTED_SPARK_PIPELINE_PATH}': {e}\")\n",
    "    print(\"Verifica que la ruta sea correcta y que el modelo se haya guardado usando pipeline_model.save().\")\n",
    "    fitted_spark_pipeline = None # Asegurarse de que la variable esté definida aunque falle la carga\n",
    "\n",
    "\n",
    "# --- Función para crear secuencias NumPy ---\n",
    "def create_sequences_np(data, sequence_length):\n",
    "    X = []\n",
    "    # Aseguramos que haya suficientes datos para al menos una secuencia\n",
    "    if len(data) < sequence_length:\n",
    "        return np.array([]).reshape(0, sequence_length, data.shape[-1]) # Devuelve array vacío con forma correcta\n",
    "\n",
    "    for i in range(len(data) - sequence_length + 1):\n",
    "        X.append(data[i:(i + sequence_length)])\n",
    "    return np.array(X)\n",
    "\n",
    "# --- Mapeo de weekday (de tu celda de ingeniería de características) ---\n",
    "weekday_mapping = {\n",
    "    \"mon\": 0, \"tue\": 1, \"wed\": 2, \"thu\": 3, \"fri\": 4, \"sat\": 5, \"sun\": 6\n",
    "}\n",
    "mapping_expr = None\n",
    "for day_str, day_num in weekday_mapping.items():\n",
    "    if mapping_expr is None:\n",
    "        mapping_expr = when(lower(col(\"weekday\")) == day_str, day_num)\n",
    "    else:\n",
    "        mapping_expr = mapping_expr.when(lower(col(\"weekday\")) == day_str, day_num)\n",
    "mapping_expr = mapping_expr.otherwise(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bf702a9-81e4-4219-b81c-f82feac6809e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1746653008.686938   14729 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1746653008.727230   14729 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1746653008.727334   14729 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1746653008.731924   14729 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1746653008.733164   14729 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1746653008.733191   14729 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1746653008.904737   14729 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1746653008.904796   14729 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-07 23:23:28.904811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1746653008.904852   14729 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-05-07 23:23:28.904876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5582 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo Keras cargado desde: models/modeloTransformer3.keras\n",
      "Forma de entrada esperada por el modelo Keras: (None, 10, 340)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "import tensorflow as tf\n",
    "\n",
    "@register_keras_serializable()\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def call(self, x):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        embedding_dim = tf.shape(x)[2]\n",
    "        pos = tf.range(seq_len, dtype=tf.float32)[:, tf.newaxis]\n",
    "        i = tf.range(embedding_dim, dtype=tf.float32)[tf.newaxis, :]\n",
    "        angle_rates = 1 / tf.pow(10000., (2 * (i // 2)) / tf.cast(embedding_dim, tf.float32))\n",
    "        angle_rads = pos * angle_rates\n",
    "        sines = tf.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[:, :embedding_dim]\n",
    "        return x + pos_encoding[tf.newaxis, :, :]\n",
    "\n",
    "# Load the model with custom objects\n",
    "custom_objects = {'PositionalEncoding': PositionalEncoding}\n",
    "model = load_model(MODEL_PATH, custom_objects=custom_objects)\n",
    "print(f\"Modelo Keras cargado desde: {MODEL_PATH}\")\n",
    "print(f\"Forma de entrada esperada por el modelo Keras: {model.input_shape}\")\n",
    "\n",
    "# Extraer el número de características esperado por el modelo Keras\n",
    "# model.input_shape es (None, TIMESTEPS, FEATURES_ESPERADAS)\n",
    "EXPECTED_NUM_FEATURES = model.input_shape[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f87d769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_scaler del entrenamiento cargado exitosamente.\n",
      "La columna objetivo (transformada) a predecir es: log_takeoff_time\n",
      "El pipeline interno espera calcular y luego limpiar 'takeoff time' a 'takeoff_time'.\n",
      "\n",
      "[1/193] Procesando escenario: scenario_001.parquet (índice df_results: 0)\n",
      "  --- Esquema de df0_spark para scenario_001.parquet (ANTES de apply_pipeline) ---\n",
      "root\n",
      " |-- Timestamp (date): timestamp_ntz (nullable = true)\n",
      " |-- ICAO: string (nullable = true)\n",
      " |-- Downlink Format: long (nullable = true)\n",
      " |-- Typecode: long (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- TurbulenceCategory: string (nullable = true)\n",
      " |-- Callsign: string (nullable = true)\n",
      " |-- Speed: double (nullable = true)\n",
      " |-- Altitude (ft): double (nullable = true)\n",
      " |-- Flight status: string (nullable = true)\n",
      "\n",
      "  Llamando a pipeline.apply_pipeline para scenario_001.parquet...\n",
      "=== INICIO apply_pipeline ===\n",
      "  Input df_input_raw count: 48028\n",
      "  DEBUG: Entrando a cleanColumns.\n",
      "  ADVERTENCIA en cleanColumns: Original 'takeoff time' para alias 'takeoff_time' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Timestamp' para alias 'timestamp' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'ICAO' para alias 'icao' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Callsign' para alias 'callsign' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Designator' para alias 'holding_point' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Runway' para alias 'runway' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'operator' para alias 'operator' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'TurbulenceCategory' para alias 'turbulence_category' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'lat' para alias 'lat' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'lon' para alias 'lon' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'last min takeoffs' para alias 'last_min_takeoffs' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'last_event' para alias 'last_event' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'last min landings' para alias 'last_min_landings' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'last_event_turb_cat' para alias 'last_event_turb_cat' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'time_since_last_event_seconds' para alias 'time_since_last_event_seconds' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'time_before_holding_point' para alias 'time_before_holding_point' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'time_at_holding_point' para alias 'time_at_holding_point' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Hour' para alias 'hour' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Weekday' para alias 'weekday' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'IsHoliday' para alias 'is_holiday' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'event_timestamp' para alias 'event_timestamp' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'first_holding_time' para alias 'first_holding_time' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'first_airborne_time' para alias 'first_airborne_time' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'first_on_ground_time' para alias 'first_on_ground_time' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z1' para alias 'Z1' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'KA6' para alias 'KA6' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'KA8' para alias 'KA8' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'K3' para alias 'K3' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'K2' para alias 'K2' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'K1' para alias 'K1' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Y1' para alias 'Y1' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Y2' para alias 'Y2' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Y3' para alias 'Y3' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Y7' para alias 'Y7' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z6' para alias 'Z6' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z4' para alias 'Z4' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z2' para alias 'Z2' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z3' para alias 'Z3' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LF' para alias 'LF' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'L1' para alias 'L1' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LA' para alias 'LA' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LB' para alias 'LB' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LC' para alias 'LC' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LD' para alias 'LD' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LE' para alias 'LE' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original '36R_18L' para alias '36R_18L' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original '32R_14L' para alias '32R_14L' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original '36L_18R' para alias '36L_18R' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original '32L_14R' para alias '32L_14R' no está. Se creará como nula.\n",
      "  DEBUG: cleanColumns - df_final_clean count: 0\n",
      "  DEBUG: Entrando a cleanColumns.\n",
      "  ADVERTENCIA en cleanColumns: Original 'takeoff time' para alias 'takeoff_time' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Timestamp' para alias 'timestamp' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'ICAO' para alias 'icao' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Callsign' para alias 'callsign' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Designator' para alias 'holding_point' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Runway' para alias 'runway' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'operator' para alias 'operator' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'TurbulenceCategory' para alias 'turbulence_category' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'lat' para alias 'lat' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'lon' para alias 'lon' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'last min takeoffs' para alias 'last_min_takeoffs' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'last_event' para alias 'last_event' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'last min landings' para alias 'last_min_landings' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'last_event_turb_cat' para alias 'last_event_turb_cat' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'time_since_last_event_seconds' para alias 'time_since_last_event_seconds' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'time_before_holding_point' para alias 'time_before_holding_point' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'time_at_holding_point' para alias 'time_at_holding_point' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Hour' para alias 'hour' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Weekday' para alias 'weekday' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'IsHoliday' para alias 'is_holiday' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'event_timestamp' para alias 'event_timestamp' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'first_holding_time' para alias 'first_holding_time' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'first_airborne_time' para alias 'first_airborne_time' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'first_on_ground_time' para alias 'first_on_ground_time' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z1' para alias 'Z1' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'KA6' para alias 'KA6' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'KA8' para alias 'KA8' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'K3' para alias 'K3' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'K2' para alias 'K2' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'K1' para alias 'K1' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Y1' para alias 'Y1' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Y2' para alias 'Y2' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Y3' para alias 'Y3' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Y7' para alias 'Y7' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z6' para alias 'Z6' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z4' para alias 'Z4' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z2' para alias 'Z2' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z3' para alias 'Z3' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LF' para alias 'LF' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'L1' para alias 'L1' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LA' para alias 'LA' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LB' para alias 'LB' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LC' para alias 'LC' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LD' para alias 'LD' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LE' para alias 'LE' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original '36R_18L' para alias '36R_18L' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original '32R_14L' para alias '32R_14L' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original '36L_18R' para alias '36L_18R' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original '32L_14R' para alias '32L_14R' no está. Se creará como nula.\n",
      "  DEBUG: cleanColumns - df_final_clean count: 0\n",
      "  1. df_pos count: 10304\n",
      "  1. df_flights count: 2608\n",
      "  1. df_types count: 106\n",
      "  1. df_speed count: 13599\n",
      "  3. df_pos_airport count: 2529\n",
      "  4. df_pos_callsign count: 2505\n",
      "  5. df_with_hp count: 2505\n",
      "  6. df_with_hp_tc count: 2505\n",
      "  7. df_valid_flights count: 298\n",
      "  8. df_takeoff_segment count: 1\n",
      "  ERROR CRÍTICO durante pipeline.apply_pipeline para scenario_001.parquet: AnalysisException - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Speed` cannot be resolved. Did you mean one of the following? [`Callsign`, `ICAO`, `Timestamp`, `Flight status`, `lat`, `lon`, `Designator`, `Runway`, `TurbulenceCategory`, `first_on_ground_time`, `first_holding_time`, `first_airborne_time`, `Timestamp_sec`].\n",
      "\n",
      "[2/193] Procesando escenario: scenario_002.parquet (índice df_results: 1)\n",
      "  --- Esquema de df0_spark para scenario_002.parquet (ANTES de apply_pipeline) ---\n",
      "root\n",
      " |-- Timestamp (date): timestamp_ntz (nullable = true)\n",
      " |-- ICAO: string (nullable = true)\n",
      " |-- Downlink Format: long (nullable = true)\n",
      " |-- Typecode: long (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- TurbulenceCategory: string (nullable = true)\n",
      " |-- Callsign: string (nullable = true)\n",
      " |-- Speed: double (nullable = true)\n",
      " |-- Altitude (ft): double (nullable = true)\n",
      " |-- Flight status: string (nullable = true)\n",
      "\n",
      "  Llamando a pipeline.apply_pipeline para scenario_002.parquet...\n",
      "=== INICIO apply_pipeline ===\n",
      "  Input df_input_raw count: 45974\n",
      "  DEBUG: Entrando a cleanColumns.\n",
      "  ADVERTENCIA en cleanColumns: Original 'takeoff time' para alias 'takeoff_time' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Timestamp' para alias 'timestamp' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'ICAO' para alias 'icao' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Callsign' para alias 'callsign' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Designator' para alias 'holding_point' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Runway' para alias 'runway' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'operator' para alias 'operator' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'TurbulenceCategory' para alias 'turbulence_category' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'lat' para alias 'lat' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'lon' para alias 'lon' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'last min takeoffs' para alias 'last_min_takeoffs' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'last_event' para alias 'last_event' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'last min landings' para alias 'last_min_landings' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'last_event_turb_cat' para alias 'last_event_turb_cat' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'time_since_last_event_seconds' para alias 'time_since_last_event_seconds' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'time_before_holding_point' para alias 'time_before_holding_point' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'time_at_holding_point' para alias 'time_at_holding_point' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Hour' para alias 'hour' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Weekday' para alias 'weekday' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'IsHoliday' para alias 'is_holiday' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'event_timestamp' para alias 'event_timestamp' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'first_holding_time' para alias 'first_holding_time' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'first_airborne_time' para alias 'first_airborne_time' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'first_on_ground_time' para alias 'first_on_ground_time' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z1' para alias 'Z1' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'KA6' para alias 'KA6' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'KA8' para alias 'KA8' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'K3' para alias 'K3' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'K2' para alias 'K2' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'K1' para alias 'K1' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Y1' para alias 'Y1' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Y2' para alias 'Y2' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Y3' para alias 'Y3' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Y7' para alias 'Y7' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z6' para alias 'Z6' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z4' para alias 'Z4' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z2' para alias 'Z2' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z3' para alias 'Z3' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LF' para alias 'LF' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'L1' para alias 'L1' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LA' para alias 'LA' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LB' para alias 'LB' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LC' para alias 'LC' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LD' para alias 'LD' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LE' para alias 'LE' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original '36R_18L' para alias '36R_18L' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original '32R_14L' para alias '32R_14L' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original '36L_18R' para alias '36L_18R' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original '32L_14R' para alias '32L_14R' no está. Se creará como nula.\n",
      "  DEBUG: cleanColumns - df_final_clean count: 0\n",
      "  DEBUG: Entrando a cleanColumns.\n",
      "  ADVERTENCIA en cleanColumns: Original 'takeoff time' para alias 'takeoff_time' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Timestamp' para alias 'timestamp' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'ICAO' para alias 'icao' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Callsign' para alias 'callsign' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Designator' para alias 'holding_point' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Runway' para alias 'runway' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'operator' para alias 'operator' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'TurbulenceCategory' para alias 'turbulence_category' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'lat' para alias 'lat' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'lon' para alias 'lon' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'last min takeoffs' para alias 'last_min_takeoffs' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'last_event' para alias 'last_event' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'last min landings' para alias 'last_min_landings' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'last_event_turb_cat' para alias 'last_event_turb_cat' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'time_since_last_event_seconds' para alias 'time_since_last_event_seconds' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'time_before_holding_point' para alias 'time_before_holding_point' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'time_at_holding_point' para alias 'time_at_holding_point' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Hour' para alias 'hour' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Weekday' para alias 'weekday' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'IsHoliday' para alias 'is_holiday' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'event_timestamp' para alias 'event_timestamp' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'first_holding_time' para alias 'first_holding_time' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'first_airborne_time' para alias 'first_airborne_time' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'first_on_ground_time' para alias 'first_on_ground_time' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z1' para alias 'Z1' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'KA6' para alias 'KA6' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'KA8' para alias 'KA8' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'K3' para alias 'K3' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'K2' para alias 'K2' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'K1' para alias 'K1' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Y1' para alias 'Y1' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Y2' para alias 'Y2' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Y3' para alias 'Y3' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Y7' para alias 'Y7' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z6' para alias 'Z6' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z4' para alias 'Z4' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z2' para alias 'Z2' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z3' para alias 'Z3' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LF' para alias 'LF' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'L1' para alias 'L1' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LA' para alias 'LA' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LB' para alias 'LB' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LC' para alias 'LC' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LD' para alias 'LD' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LE' para alias 'LE' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original '36R_18L' para alias '36R_18L' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original '32R_14L' para alias '32R_14L' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original '36L_18R' para alias '36L_18R' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original '32L_14R' para alias '32L_14R' no está. Se creará como nula.\n",
      "  DEBUG: cleanColumns - df_final_clean count: 0\n",
      "  1. df_pos count: 10726\n",
      "  1. df_flights count: 2702\n",
      "  1. df_types count: 109\n",
      "  1. df_speed count: 13874\n",
      "  3. df_pos_airport count: 3376\n",
      "  4. df_pos_callsign count: 3375\n",
      "  5. df_with_hp count: 3375\n",
      "  6. df_with_hp_tc count: 3375\n",
      "  7. df_valid_flights count: 264\n",
      "  8. df_takeoff_segment count: 1\n",
      "  ERROR CRÍTICO durante pipeline.apply_pipeline para scenario_002.parquet: AnalysisException - [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Speed` cannot be resolved. Did you mean one of the following? [`Callsign`, `ICAO`, `Timestamp`, `Flight status`, `lat`, `lon`, `Designator`, `Runway`, `TurbulenceCategory`, `first_on_ground_time`, `first_holding_time`, `first_airborne_time`, `Timestamp_sec`].\n",
      "\n",
      "[3/193] Procesando escenario: scenario_003.parquet (índice df_results: 2)\n",
      "  --- Esquema de df0_spark para scenario_003.parquet (ANTES de apply_pipeline) ---\n",
      "root\n",
      " |-- Timestamp (date): timestamp_ntz (nullable = true)\n",
      " |-- ICAO: string (nullable = true)\n",
      " |-- Downlink Format: long (nullable = true)\n",
      " |-- Typecode: long (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- TurbulenceCategory: string (nullable = true)\n",
      " |-- Callsign: string (nullable = true)\n",
      " |-- Speed: double (nullable = true)\n",
      " |-- Altitude (ft): double (nullable = true)\n",
      " |-- Flight status: string (nullable = true)\n",
      "\n",
      "  Llamando a pipeline.apply_pipeline para scenario_003.parquet...\n",
      "=== INICIO apply_pipeline ===\n",
      "  Input df_input_raw count: 41347\n",
      "  DEBUG: Entrando a cleanColumns.\n",
      "  ADVERTENCIA en cleanColumns: Original 'takeoff time' para alias 'takeoff_time' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Timestamp' para alias 'timestamp' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'ICAO' para alias 'icao' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Callsign' para alias 'callsign' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Designator' para alias 'holding_point' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Runway' para alias 'runway' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'operator' para alias 'operator' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'TurbulenceCategory' para alias 'turbulence_category' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'lat' para alias 'lat' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'lon' para alias 'lon' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'last min takeoffs' para alias 'last_min_takeoffs' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'last_event' para alias 'last_event' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'last min landings' para alias 'last_min_landings' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'last_event_turb_cat' para alias 'last_event_turb_cat' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'time_since_last_event_seconds' para alias 'time_since_last_event_seconds' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'time_before_holding_point' para alias 'time_before_holding_point' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'time_at_holding_point' para alias 'time_at_holding_point' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Hour' para alias 'hour' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Weekday' para alias 'weekday' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'IsHoliday' para alias 'is_holiday' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'event_timestamp' para alias 'event_timestamp' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'first_holding_time' para alias 'first_holding_time' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'first_airborne_time' para alias 'first_airborne_time' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'first_on_ground_time' para alias 'first_on_ground_time' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z1' para alias 'Z1' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'KA6' para alias 'KA6' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'KA8' para alias 'KA8' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'K3' para alias 'K3' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'K2' para alias 'K2' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'K1' para alias 'K1' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Y1' para alias 'Y1' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Y2' para alias 'Y2' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Y3' para alias 'Y3' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Y7' para alias 'Y7' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z6' para alias 'Z6' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z4' para alias 'Z4' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z2' para alias 'Z2' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z3' para alias 'Z3' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LF' para alias 'LF' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'L1' para alias 'L1' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LA' para alias 'LA' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LB' para alias 'LB' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LC' para alias 'LC' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LD' para alias 'LD' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LE' para alias 'LE' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original '36R_18L' para alias '36R_18L' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original '32R_14L' para alias '32R_14L' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original '36L_18R' para alias '36L_18R' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original '32L_14R' para alias '32L_14R' no está. Se creará como nula.\n",
      "  DEBUG: cleanColumns - df_final_clean count: 0\n",
      "  DEBUG: Entrando a cleanColumns.\n",
      "  ADVERTENCIA en cleanColumns: Original 'takeoff time' para alias 'takeoff_time' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Timestamp' para alias 'timestamp' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'ICAO' para alias 'icao' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Callsign' para alias 'callsign' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Designator' para alias 'holding_point' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Runway' para alias 'runway' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'operator' para alias 'operator' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'TurbulenceCategory' para alias 'turbulence_category' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'lat' para alias 'lat' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'lon' para alias 'lon' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'last min takeoffs' para alias 'last_min_takeoffs' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'last_event' para alias 'last_event' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'last min landings' para alias 'last_min_landings' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'last_event_turb_cat' para alias 'last_event_turb_cat' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'time_since_last_event_seconds' para alias 'time_since_last_event_seconds' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'time_before_holding_point' para alias 'time_before_holding_point' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'time_at_holding_point' para alias 'time_at_holding_point' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Hour' para alias 'hour' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Weekday' para alias 'weekday' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'IsHoliday' para alias 'is_holiday' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'event_timestamp' para alias 'event_timestamp' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'first_holding_time' para alias 'first_holding_time' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'first_airborne_time' para alias 'first_airborne_time' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'first_on_ground_time' para alias 'first_on_ground_time' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z1' para alias 'Z1' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'KA6' para alias 'KA6' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'KA8' para alias 'KA8' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'K3' para alias 'K3' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'K2' para alias 'K2' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'K1' para alias 'K1' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Y1' para alias 'Y1' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Y2' para alias 'Y2' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Y3' para alias 'Y3' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Y7' para alias 'Y7' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z6' para alias 'Z6' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z4' para alias 'Z4' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z2' para alias 'Z2' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'Z3' para alias 'Z3' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LF' para alias 'LF' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'L1' para alias 'L1' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LA' para alias 'LA' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LB' para alias 'LB' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LC' para alias 'LC' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LD' para alias 'LD' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original 'LE' para alias 'LE' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original '36R_18L' para alias '36R_18L' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original '32R_14L' para alias '32R_14L' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original '36L_18R' para alias '36L_18R' no está. Se creará como nula.\n",
      "  ADVERTENCIA en cleanColumns: Original '32L_14R' para alias '32L_14R' no está. Se creará como nula.\n",
      "  DEBUG: cleanColumns - df_final_clean count: 0\n",
      "  1. df_pos count: 9457\n",
      "  1. df_flights count: 2324\n",
      "  1. df_types count: 93\n",
      "  1. df_speed count: 12275\n",
      "  3. df_pos_airport count: 2503\n",
      "  4. df_pos_callsign count: 2502\n",
      "  5. df_with_hp count: 2502\n",
      "  6. df_with_hp_tc count: 2502\n",
      "  7. df_valid_flights count: 222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bryan/pruebas/.venv/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bryan/pruebas/.venv/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/socket.py\", line 718, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Llamando a pipeline.apply_pipeline para \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Aquí es donde ocurre el AnalysisException para scenario_002 si 'takeoff time' no se genera\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m df1_spark = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf0_spark\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m df1_spark \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     64\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ERROR: pipeline.apply_pipeline devolvió None para \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pruebas/despegues/src/preprocess/pipeline.py:738\u001b[39m, in \u001b[36mPipeline.apply_pipeline\u001b[39m\u001b[34m(self, df_input_raw)\u001b[39m\n\u001b[32m    735\u001b[39m df_valid_flights = \u001b[38;5;28mself\u001b[39m.filterFlights(df_with_hp_tc); \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  7. df_valid_flights count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_valid_flights.count()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m df_valid_flights.isEmpty(): \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  EARLY EXIT: df_valid_flights vacío.\u001b[39m\u001b[33m\"\u001b[39m); \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.spark.createDataFrame([], final_empty_schema)\n\u001b[32m--> \u001b[39m\u001b[32m738\u001b[39m df_takeoff_segment = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilterPositions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_valid_flights\u001b[49m\u001b[43m)\u001b[49m; \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  8. df_takeoff_segment count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_takeoff_segment.count()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m df_takeoff_segment.isEmpty(): \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  EARLY EXIT: df_takeoff_segment vacío.\u001b[39m\u001b[33m\"\u001b[39m); \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.spark.createDataFrame([], final_empty_schema)\n\u001b[32m    741\u001b[39m df_with_velocities = \u001b[38;5;28mself\u001b[39m.mergePositionsVelocities(df_takeoff_segment, df_speed); \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  9. df_with_velocities count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_with_velocities.count()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pruebas/despegues/src/preprocess/pipeline.py:246\u001b[39m, in \u001b[36mPipeline.filterPositions\u001b[39m\u001b[34m(self, df_valid_flights)\u001b[39m\n\u001b[32m    241\u001b[39m df_with_first_holding = df_with_first_on_ground.withColumn(\n\u001b[32m    242\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfirst_holding_time\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    243\u001b[39m     F_min(when(col(\u001b[33m\"\u001b[39m\u001b[33mDesignator\u001b[39m\u001b[33m\"\u001b[39m).isNotNull(), col(\u001b[33m\"\u001b[39m\u001b[33mTimestamp\u001b[39m\u001b[33m\"\u001b[39m))).over(window_spec)\n\u001b[32m    244\u001b[39m )\n\u001b[32m    245\u001b[39m df_from_hp_base = df_with_first_holding.filter(col(\u001b[33m\"\u001b[39m\u001b[33mfirst_holding_time\u001b[39m\u001b[33m\"\u001b[39m).isNotNull())\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdf_from_hp_base\u001b[49m\u001b[43m.\u001b[49m\u001b[43misEmpty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m: \u001b[38;5;28;01mreturn\u001b[39;00m df_from_hp_base \u001b[38;5;66;03m# No flights reached holding or no timestamp for it\u001b[39;00m\n\u001b[32m    248\u001b[39m df_from_hp = df_from_hp_base.filter(col(\u001b[33m\"\u001b[39m\u001b[33mTimestamp\u001b[39m\u001b[33m\"\u001b[39m) >= col(\u001b[33m\"\u001b[39m\u001b[33mfirst_holding_time\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m df_from_hp.isEmpty(): \u001b[38;5;28;01mreturn\u001b[39;00m df_from_hp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pruebas/.venv/lib/python3.11/site-packages/pyspark/sql/dataframe.py:885\u001b[39m, in \u001b[36mDataFrame.isEmpty\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34misEmpty\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    837\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    838\u001b[39m \u001b[33;03m    Checks if the :class:`DataFrame` is empty and returns a boolean value.\u001b[39;00m\n\u001b[32m    839\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    883\u001b[39m \u001b[33;03m    True\u001b[39;00m\n\u001b[32m    884\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m885\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43misEmpty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pruebas/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1314\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m return_value = get_return_value(\n\u001b[32m   1323\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pruebas/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1036\u001b[39m connection = \u001b[38;5;28mself\u001b[39m._get_connection()\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m._create_connection_guard(connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pruebas/.venv/lib/python3.11/site-packages/py4j/clientserver.py:511\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m         answer = smart_decode(\u001b[38;5;28mself\u001b[39m.stream.readline()[:-\u001b[32m1\u001b[39m])\n\u001b[32m    512\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m\"\u001b[39m.format(answer))\n\u001b[32m    513\u001b[39m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[32m    514\u001b[39m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/socket.py:718\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    720\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from pyspark.sql.functions import col, log1p, sin, lit, cos, pi, monotonically_increasing_id\n",
    "# Asegúrate de que 'spark', 'pipeline', 'add_meteo_data', 'METEO_PATH',\n",
    "# 'mapping_expr', 'fitted_spark_pipeline', 'create_sequences_np', 'model',\n",
    "# 'EXPECTED_NUM_FEATURES', 'df_results', 'RESULT_PATH', 'DIR' están definidos e importados.\n",
    "# Ejemplo: from despegues.src.preprocess.pipeline import Pipeline (o como se llame tu clase)\n",
    "#          pipeline = Pipeline()\n",
    "\n",
    "\n",
    "# --- Carga el y_scaler ajustado durante el entrenamiento ---\n",
    "try:\n",
    "    y_scaler_trained = joblib.load('models/completo/y_scaler_trained_FINAL.gz')\n",
    "    print(\"y_scaler del entrenamiento cargado exitosamente.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: No se encontró el archivo 'y_scaler_trained_FINAL.gz'.\")\n",
    "    print(\"Continuando sin y_scaler, las predicciones no se podrán desescalar correctamente.\")\n",
    "    y_scaler_trained = None\n",
    "    \n",
    "TARGET_COL = \"log_takeoff_time\" # Columna objetivo que el modelo predice (transformada)\n",
    "RAW_TARGET_COL_NAME_IN_PIPELINE = \"takeoff time\" # Nombre esperado por cleanColumns (con espacio)\n",
    "RAW_TARGET_COL_NAME_SPARK = \"takeoff_time\"       # Nombre después de cleanColumns (estándar Spark)\n",
    "\n",
    "SEQUENCE_LENGTH = 10\n",
    "print(f\"La columna objetivo (transformada) a predecir es: {TARGET_COL}\")\n",
    "print(f\"El pipeline interno espera calcular y luego limpiar '{RAW_TARGET_COL_NAME_IN_PIPELINE}' a '{RAW_TARGET_COL_NAME_SPARK}'.\")\n",
    "\n",
    "# Iterar sobre las filas de df_results para asegurar consistencia\n",
    "# Asumimos que df_results es un Pandas DataFrame con una columna 'scenario_name'\n",
    "# y que su índice es el que queremos usar para actualizar.\n",
    "for idx in range(3):\n",
    "    # Usamos el índice numérico para iloc para obtener el nombre del escenario,\n",
    "    # pero el índice real de la fila para .loc para actualizar.\n",
    "    # Esto asume que el índice de df_results es el estándar 0, 1, 2...\n",
    "    # Si df_results tiene un índice personalizado, ajústalo o usa df_results.index[idx]\n",
    "    current_df_results_index = df_results.index[idx]\n",
    "    scenario_name_from_df = df_results.loc[current_df_results_index, 'scenario_name']\n",
    "    \n",
    "    file_name = scenario_name_from_df # Asume que scenario_name es el nombre del archivo .parquet\n",
    "    file_path = os.path.join(DIR, file_name)\n",
    "    \n",
    "    print(f\"\\n[{idx + 1}/{len(df_results)}] Procesando escenario: {file_name} (índice df_results: {current_df_results_index})\")\n",
    "    df_results.loc[current_df_results_index, \"time_to_takeoff_s\"] = -99 # Valor por defecto para indicar no procesado / error\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"  ERROR CRÍTICO: Archivo no encontrado: {file_path}\")\n",
    "        df_results.loc[current_df_results_index, \"time_to_takeoff_s\"] = -1 # Código de error: Archivo no encontrado\n",
    "        continue\n",
    "\n",
    "    # Cargar datos brutos del escenario (Spark DataFrame)\n",
    "    df0_spark = spark.read.parquet(file_path)\n",
    "    print(f\"  --- Esquema de df0_spark para {file_name} (ANTES de apply_pipeline) ---\")\n",
    "    df0_spark.printSchema(level=1)\n",
    "\n",
    "    # Aplicar pipeline custom de procesamiento (Spark)\n",
    "    df1_spark = None\n",
    "    try:\n",
    "        print(f\"  Llamando a pipeline.apply_pipeline para {file_name}...\")\n",
    "        # Aquí es donde ocurre el AnalysisException para scenario_002 si 'takeoff time' no se genera\n",
    "        df1_spark = pipeline.apply_pipeline(df0_spark) \n",
    "        \n",
    "        if df1_spark is None:\n",
    "            print(f\"  ERROR: pipeline.apply_pipeline devolvió None para {file_name}\")\n",
    "            df_results.loc[current_df_results_index, \"time_to_takeoff_s\"] = -2 # Código de error: Pipeline devolvió None\n",
    "            continue\n",
    "        \n",
    "        print(f\"  --- Esquema de df1_spark para {file_name} (DESPUÉS de apply_pipeline) ---\")\n",
    "        df1_spark.printSchema(level=1)\n",
    "        # df1_spark.show(5, truncate=False) # Descomentar para ver algunas filas\n",
    "\n",
    "        # Verificar si la columna renombrada RAW_TARGET_COL_NAME_SPARK ('takeoff_time') está presente\n",
    "        if RAW_TARGET_COL_NAME_SPARK not in df1_spark.columns:\n",
    "            print(f\"  ERROR CRÍTICO: La columna '{RAW_TARGET_COL_NAME_SPARK}' NO está en el DataFrame después de pipeline.apply_pipeline para {file_name}.\")\n",
    "            print(f\"  Esto significa que '{RAW_TARGET_COL_NAME_IN_PIPELINE}' no se pudo calcular o renombrar correctamente dentro del pipeline.\")\n",
    "            df_results.loc[current_df_results_index, \"time_to_takeoff_s\"] = -3 # Código de error: Target col faltante post-pipeline\n",
    "            continue\n",
    "\n",
    "        if df1_spark.count() == 0:\n",
    "            print(f\"  ADVERTENCIA: pipeline.apply_pipeline devolvió un DataFrame vacío para {file_name}\")\n",
    "            df_results.loc[current_df_results_index, \"time_to_takeoff_s\"] = -4 # Código de error: Pipeline devolvió DF vacío\n",
    "            continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR CRÍTICO durante pipeline.apply_pipeline para {file_name}: {type(e).__name__} - {e}\")\n",
    "        import traceback\n",
    "        # traceback.print_exc() # Descomentar para traceback completo si es necesario\n",
    "        df_results.loc[current_df_results_index, \"time_to_takeoff_s\"] = -5 # Código de error: Excepción en pipeline\n",
    "        continue\n",
    "        \n",
    "    df2_spark = add_meteo_data(df1_spark, METEO_PATH)\n",
    "    if df2_spark is None or df2_spark.count() == 0:\n",
    "        print(f\"  ADVERTENCIA: add_meteo_data devolvió un DataFrame vacío o nulo para {file_name}\")\n",
    "        df_results.loc[current_df_results_index, \"time_to_takeoff_s\"] = -6 # Código de error: Meteo devolvió DF vacío/nulo\n",
    "        continue\n",
    "\n",
    "    # --- Creación de TARGET_COL (log_takeoff_time) para posible evaluación ---\n",
    "    # En inferencia pura, no necesitas y_test_raw. Si 'takeoff_time' (la columna original)\n",
    "    # está presente, podemos crear la versión logarítmica para y_test_raw.\n",
    "    # Si no está (lo cual sería extraño si el pipeline la generó), y_test_raw no se creará.\n",
    "    can_create_log_target_for_eval = False\n",
    "    if RAW_TARGET_COL_NAME_SPARK in df2_spark.columns:\n",
    "        df2_spark = df2_spark.withColumn(TARGET_COL, log1p(col(RAW_TARGET_COL_NAME_SPARK)))\n",
    "        can_create_log_target_for_eval = True\n",
    "        print(f\"  Columna '{TARGET_COL}' creada a partir de '{RAW_TARGET_COL_NAME_SPARK}' para evaluación.\")\n",
    "    else:\n",
    "        # Esto no debería ocurrir si la comprobación anterior en df1_spark pasó y add_meteo_data no la eliminó.\n",
    "        print(f\"  ADVERTENCIA: '{RAW_TARGET_COL_NAME_SPARK}' no encontrada en df2_spark (post-meteo) para {file_name}. No se creará '{TARGET_COL}'.\")\n",
    "\n",
    "    df2_spark = df2_spark.withColumn(\"weekday_num\", mapping_expr)\n",
    "    df2_spark = df2_spark.withColumn(\"hour_sin\", sin(lit(2) * pi() * col(\"hour\") / lit(24.0)))\n",
    "    df2_spark = df2_spark.withColumn(\"hour_cos\", cos(lit(2) * pi() * col(\"hour\") / lit(24.0)))\n",
    "    df2_spark = df2_spark.withColumn(\"weekday_sin\", sin(lit(2) * pi() * col(\"weekday_num\") / lit(7.0)))\n",
    "    df2_spark = df2_spark.withColumn(\"weekday_cos\", cos(lit(2) * pi() * col(\"weekday_num\") / lit(7.0)))\n",
    "    df2_spark = df2_spark.withColumn(\"__index_level_0__\", monotonically_increasing_id().cast(\"long\"))\n",
    "\n",
    "    print(f\"  --- Esquema de df2_spark para {file_name} (antes de fitted_spark_pipeline.transform()) ---\")\n",
    "    df2_spark.printSchema(level=1)\n",
    "\n",
    "    df_model_input_spark = fitted_spark_pipeline.transform(df2_spark)\n",
    "    df_model_input_pandas = df_model_input_spark.toPandas()\n",
    "\n",
    "    if df_model_input_pandas.empty or 'features' not in df_model_input_pandas.columns:\n",
    "        print(f\"  ADVERTENCIA: DataFrame para modelo (pandas) está vacío o falta 'features' para {file_name}.\")\n",
    "        df_results.loc[current_df_results_index, \"time_to_takeoff_s\"] = -7 # Código de error: Pandas DF vacío/sin features\n",
    "        continue\n",
    "\n",
    "    features_list = df_model_input_pandas['features'].apply(lambda x: x.toArray().tolist()).tolist()\n",
    "    features_np = np.array(features_list)\n",
    "\n",
    "    y_test_raw = None # Para evaluación, no estrictamente necesario para predicción\n",
    "    if can_create_log_target_for_eval and TARGET_COL in df_model_input_pandas.columns:\n",
    "        y_test_raw = df_model_input_pandas[TARGET_COL].values\n",
    "        print(f\"  Extraído y_test_raw (shape: {y_test_raw.shape}) para {file_name}.\")\n",
    "    elif TARGET_COL not in df_model_input_pandas.columns and can_create_log_target_for_eval:\n",
    "         print(f\"  ADVERTENCIA: '{TARGET_COL}' no encontrada en df_model_input_pandas aunque se esperaba. No se puede crear y_test_raw para {file_name}.\")\n",
    "    else:\n",
    "        print(f\"  Info: No se creó '{TARGET_COL}' o no está en pandas DF, y_test_raw no disponible para {file_name}.\")\n",
    "\n",
    "    if features_np.ndim == 1:\n",
    "        features_np = features_np.reshape(1, -1) if features_np.shape[0] > 0 else np.array([]).reshape(0,0) # Manejar array vacío\n",
    "\n",
    "    if features_np.shape[0] == 0:\n",
    "        print(f\"  ADVERTENCIA: features_np tiene 0 filas para {file_name} (antes de ajuste dim).\")\n",
    "        df_results.loc[current_df_results_index, \"time_to_takeoff_s\"] = -8 # Código de error: features_np vacío\n",
    "        continue\n",
    "        \n",
    "    current_num_features = features_np.shape[1]\n",
    "    print(f\"  features_np shape (antes ajuste dim): {features_np.shape}. Contiene NaN: {np.isnan(features_np).any()}, Inf: {np.isinf(features_np).any()}\")\n",
    "    if features_np.size > 0: print(f\"    Min: {np.min(features_np):.4f}, Max: {np.max(features_np):.4f}, Mean: {np.mean(features_np):.4f}\")\n",
    "\n",
    "    if current_num_features != EXPECTED_NUM_FEATURES:\n",
    "        print(f\"  Ajustando características: {current_num_features} -> {EXPECTED_NUM_FEATURES}. Relleno/Truncado.\")\n",
    "        if current_num_features < EXPECTED_NUM_FEATURES:\n",
    "            padding = np.zeros((features_np.shape[0], EXPECTED_NUM_FEATURES - current_num_features))\n",
    "            features_np = np.concatenate((features_np, padding), axis=1)\n",
    "        else:\n",
    "            features_np = features_np[:, :EXPECTED_NUM_FEATURES]\n",
    "        print(f\"  features_np shape (después ajuste dim): {features_np.shape}. NaN: {np.isnan(features_np).any()}, Inf: {np.isinf(features_np).any()}\")\n",
    "        if features_np.size > 0: print(f\"    Min: {np.min(features_np):.4f}, Max: {np.max(features_np):.4f}, Mean: {np.mean(features_np):.4f}\")\n",
    "    \n",
    "    X_scenario_seq = create_sequences_np(features_np, SEQUENCE_LENGTH)\n",
    "    \n",
    "    if X_scenario_seq.shape[0] > 0:\n",
    "        print(f\"  X_scenario_seq shape: {X_scenario_seq.shape}. NaN: {np.isnan(X_scenario_seq).any()}, Inf: {np.isinf(X_scenario_seq).any()}\")\n",
    "        if np.isnan(X_scenario_seq).any() or np.isinf(X_scenario_seq).any():\n",
    "            print(f\"  ADVERTENCIA CRÍTICA: X_scenario_seq para {file_name} contiene NaN/Inf ANTES de model.predict(). ¡Esto causará NaN en la predicción!\")\n",
    "            # Aquí podrías añadir lógica para guardar X_scenario_seq o features_np para inspección\n",
    "            # np.save(f\"debug_X_scenario_seq_{scenario_name_from_df}.npy\", X_scenario_seq)\n",
    "            # np.save(f\"debug_features_np_{scenario_name_from_df}.npy\", features_np)\n",
    "            df_results.loc[current_df_results_index, \"time_to_takeoff_s\"] = -9 # Error: NaN/Inf en input del modelo\n",
    "            continue\n",
    "\n",
    "        scenario_predictions_scaled = model.predict(X_scenario_seq)\n",
    "\n",
    "        if np.isnan(scenario_predictions_scaled).any():\n",
    "            print(f\"  ADVERTENCIA: Predicciones escaladas del modelo (model.predict) contienen NaN para {file_name}.\")\n",
    "            last_prediction_for_scenario = np.nan \n",
    "        elif y_scaler_trained is not None:\n",
    "            # Asegurar forma 2D para inverse_transform\n",
    "            if scenario_predictions_scaled.ndim == 1:\n",
    "                 scenario_predictions_scaled = scenario_predictions_scaled.reshape(-1, 1)\n",
    "            \n",
    "            # Verificar compatibilidad de features con y_scaler_trained\n",
    "            expected_scaler_features = getattr(y_scaler_trained, 'n_features_in_', 1) # Default a 1 si no está el atributo\n",
    "            if scenario_predictions_scaled.shape[1] != expected_scaler_features:\n",
    "                 print(f\"  ADVERTENCIA: Discrepancia en features para y_scaler_trained. Pred. shape: {scenario_predictions_scaled.shape}, y_scaler espera: {expected_scaler_features}\")\n",
    "                 if scenario_predictions_scaled.shape[1] > 0 : # Intentar tomar la primera columna\n",
    "                    scenario_predictions_scaled = scenario_predictions_scaled[:,0].reshape(-1,1)\n",
    "                 else: # No se puede resolver\n",
    "                    last_prediction_for_scenario = np.nan\n",
    "                    df_results.loc[current_df_results_index, \"time_to_takeoff_s\"] = last_prediction_for_scenario\n",
    "                    continue\n",
    "            \n",
    "            try:\n",
    "                scenario_predictions_log = y_scaler_trained.inverse_transform(scenario_predictions_scaled).flatten()\n",
    "                scenario_predictions_orig = np.expm1(scenario_predictions_log)\n",
    "\n",
    "                if scenario_predictions_orig.shape[0] > 0:\n",
    "                    last_prediction_for_scenario = scenario_predictions_orig[-1]\n",
    "                    if np.isnan(last_prediction_for_scenario):\n",
    "                        print(f\"  ADVERTENCIA: Predicción final es NaN después de inverse_transform/expm1 para {file_name}.\")\n",
    "                    else:\n",
    "                        print(f\"  PREDICCIÓN para {file_name} (último timestamp): {last_prediction_for_scenario:.2f} segundos\")\n",
    "                else:\n",
    "                    print(f\"  ADVERTENCIA: No se generaron predicciones (orig) después de invertir escalado para {file_name}.\")\n",
    "                    last_prediction_for_scenario = -10 # Error específico\n",
    "            except Exception as e_inv_transform:\n",
    "                print(f\"  ERROR durante inverse_transform/expm1 para {file_name}: {e_inv_transform}\")\n",
    "                last_prediction_for_scenario = np.nan\n",
    "\n",
    "        else: # y_scaler_trained is None\n",
    "            print(f\"  ADVERTENCIA: y_scaler_trained no disponible. Usando predicción escalada como resultado para {file_name}.\")\n",
    "            last_prediction_for_scenario = scenario_predictions_scaled.flatten()[-1] if scenario_predictions_scaled.size > 0 else -11\n",
    "\n",
    "        df_results.loc[current_df_results_index, \"time_to_takeoff_s\"] = last_prediction_for_scenario\n",
    "    else:\n",
    "        print(f\"  ADVERTENCIA: No hay suficientes datos en {file_name} para crear secuencias (X_scenario_seq vacío).\")\n",
    "        df_results.loc[current_df_results_index, \"time_to_takeoff_s\"] = -12 \n",
    "\n",
    "# --- Guardar el dataframe de resultados final ---\n",
    "df_results.to_csv(RESULT_PATH, index=False)\n",
    "print(f\"\\nResultados guardados en: {RESULT_PATH}\")\n",
    "df_results.info() # Muestra un resumen de los resultados, incluyendo conteo de nulos si las predicciones fueron NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d070934a",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f53af0a",
   "metadata": {},
   "source": [
    "Importamos el modelo final de la fase de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a716afdb",
   "metadata": {},
   "source": [
    "Realizamos las predicciones sobre el escenario procesado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afeeacb",
   "metadata": {},
   "source": [
    "Guardamos la predicción correspondiente en el CSV de respuestas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fffc379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------\n",
    "# Mis pruebecitas\n",
    "#----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea6f171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo hago primero con uno y luego hago el bucle\n",
    "import pandas as pd\n",
    "\n",
    "RESULT_PATH = \"/Users/maria/Dropbox/UCM/PD2/despegues/src/data/final_scenarios/answers_empty.csv\"\n",
    "#MODEL_PATH = \n",
    "df_results = pd.read_csv(RESULT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8367a3c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8698e171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|  ICAO|  TurbulenceCategory|\n",
      "+------+--------------------+\n",
      "|ab7376|Heavy (larger tha...|\n",
      "|344099|Medium 2 (between...|\n",
      "|3420cc|Medium 2 (between...|\n",
      "|346303|Medium 2 (between...|\n",
      "|39ceb0|Medium 2 (between...|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = getAirplaneCategories(df0)\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ceab76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
